---
title: Latest 15 Papers - November 13, 2025
labels: documentation
---
**Please check the [Github](https://github.com/Ed1sonChen/DailyArxiv) page for a better reading experience and more papers.**

## Vision Language Action
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Unified Vision-Language-Action Model](https://arxiv.org/pdf/2506.19850v1)** | 2025-06-25 | technical report |
| **[A Survey on Vision-Language-Action Models: An Action Tokenization Perspective](https://arxiv.org/pdf/2507.01925v1)** | 2025-07-03 | 70 pages, 5 figures |
| **[ROSA: Harnessing Robot States for Vision-Language and Action Alignment](https://arxiv.org/pdf/2506.13679v1)** | 2025-06-17 |  |
| **[CoVLA: Comprehensive Vision-Language-Action Dataset for Autonomous Driving](https://arxiv.org/pdf/2408.10845v3)** | 2025-10-15 | <details><summary>WACV ...</summary><p>WACV 2025, Project Page: https://turingmotors.github.io/covla-ad/</p></details> |
| **[Vision-Language-Action Models: Concepts, Progress, Applications and Challenges](https://arxiv.org/pdf/2505.04769v1)** | 2025-05-09 | <details><summary>36 pa...</summary><p>36 pages, 18 Figures, 4 Tables</p></details> |
| **[LLaDA-VLA: Vision Language Diffusion Action Models](https://arxiv.org/pdf/2509.06932v2)** | 2025-09-11 |  |
| **[InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation](https://arxiv.org/pdf/2507.17520v1)** | 2025-07-24 | 38 pages |
| **[Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations](https://arxiv.org/pdf/2509.11417v2)** | 2025-09-18 | <details><summary>Proje...</summary><p>Project Page: https://gen-vla.github.io/</p></details> |
| **[NaVILA: Legged Robot Vision-Language-Action Model for Navigation](https://arxiv.org/pdf/2412.04453v2)** | 2025-02-18 | <details><summary>Websi...</summary><p>Website: https://navila-bot.github.io/</p></details> |
| **[Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning](https://arxiv.org/pdf/2510.11027v1)** | 2025-10-14 |  |
| **[GeoVLA: Empowering 3D Representations in Vision-Language-Action Models](https://arxiv.org/pdf/2508.09071v2)** | 2025-08-14 | <details><summary>The p...</summary><p>The project is visible at https://linsun449.github.io/GeoVLA/</p></details> |
| **[Can Vision Language Models Understand Mimed Actions?](https://arxiv.org/pdf/2506.21586v2)** | 2025-08-08 | ACL 2025 Findings |
| **[LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments](https://arxiv.org/pdf/2510.19655v1)** | 2025-10-23 |  |
| **[A3VLM: Actionable Articulation-Aware Vision Language Model](https://arxiv.org/pdf/2406.07549v2)** | 2024-06-14 |  |
| **[InSpire: Vision-Language-Action Models with Intrinsic Spatial Reasoning](https://arxiv.org/pdf/2505.13888v3)** | 2025-09-30 |  |

## robot
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Co-Robots as Care Robots](https://arxiv.org/pdf/2004.04374v1)** | 2020-04-10 | <details><summary>Accep...</summary><p>Accepted paper of the AAAI 2020 Spring Symposium "Applied AI in Healthcare: Safety, Community, and the Environment" (Stanford University). Because of the COVID-19 outbreak, the physical meeting has been postponed</p></details> |
| **[Robot Accident Investigation: a case study in Responsible Robotics](https://arxiv.org/pdf/2005.07474v1)** | 2020-05-18 |  |
| **[Robotics CTF (RCTF), a playground for robot hacking](https://arxiv.org/pdf/1810.02690v4)** | 2021-11-16 |  |
| **[A Review on Robot Manipulation Methods in Human-Robot Interactions](https://arxiv.org/pdf/2309.04687v1)** | 2023-09-12 |  |
| **[Robots as Actors in a Film: No War, A Robot Story](https://arxiv.org/pdf/1910.12294v1)** | 2019-10-29 |  |
| **[Soft is Safe: Human-Robot Interaction for Soft Robots](https://arxiv.org/pdf/2502.01256v1)** | 2025-02-04 | 53 pages, 3 figures |
| **[Robots Pre-train Robots: Manipulation-Centric Robotic Representation from Large-Scale Robot Datasets](https://arxiv.org/pdf/2410.22325v2)** | 2024-10-31 |  |
| **[Automatically designing robot swarms in environments populated by other robots: an experiment in robot shepherding](https://arxiv.org/pdf/2404.18221v1)** | 2024-04-30 | <details><summary>2024 ...</summary><p>2024 IEEE International Conference on Robotics and Automation (ICRA)</p></details> |
| **[Mobile Robot Yielding Cues for Human-Robot Spatial Interaction](https://arxiv.org/pdf/2104.02279v1)** | 2021-04-07 | <details><summary>6 pag...</summary><p>6 pages, 5 figures. Submitted to IEEE Robotics and Automation Letters and 2021 IEEE/RSJ International Conference on Robotics and Automation (IROS'21)</p></details> |
| **[Malleable Robots](https://arxiv.org/pdf/2502.04012v1)** | 2025-02-07 | <details><summary>37 pa...</summary><p>37 pages, 22 figures, chapter 7 of "Handbook on Soft Robotics"</p></details> |
| **[Robot Operating System Compatible Mobile Robots for Education and Research](https://arxiv.org/pdf/2012.12527v1)** | 2020-12-24 | <details><summary>13 Pa...</summary><p>13 Pages, 22 Figures, Provided by Inovasyon Muhendislik</p></details> |
| **[Concerns and Values in Human-Robot Interactions: A Focus on Social Robotics](https://arxiv.org/pdf/2501.05628v1)** | 2025-01-13 | <details><summary>52 pa...</summary><p>52 pages, 10 figures, 5 appendices</p></details> |
| **[MobiKa - Low-Cost Mobile Robot for Human-Robot Interaction](https://arxiv.org/pdf/1905.01065v1)** | 2024-04-24 |  |
| **[Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics](https://arxiv.org/pdf/2506.00070v1)** | 2025-06-03 | 26 pages, 14 figures |
| **[On Robot Revolution and Taxation](https://arxiv.org/pdf/1808.01666v1)** | 2018-08-07 |  |

## Vision Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Assessing Color Vision Test in Large Vision-language Models](https://arxiv.org/pdf/2507.11153v1)** | 2025-07-16 |  |
| **[Vision-and-Language Pretrained Models: A Survey](https://arxiv.org/pdf/2204.07356v5)** | 2022-05-05 | <details><summary>Accep...</summary><p>Accepted in IJCAI 2022</p></details> |
| **[Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks](https://arxiv.org/pdf/2301.05065v2)** | 2023-10-18 |  |
| **[Unveiling Encoder-Free Vision-Language Models](https://arxiv.org/pdf/2406.11832v2)** | 2024-10-30 | <details><summary>17 pa...</summary><p>17 pages, 8 figures, Accepted by NeurIPS2024 (spotlight)</p></details> |
| **[FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/pdf/2112.04482v3)** | 2022-03-31 | CVPR 2022 |
| **[HiMix: Reducing Computational Complexity in Large Vision-Language Models](https://arxiv.org/pdf/2501.10318v1)** | 2025-01-20 |  |
| **[An Introduction to Vision-Language Modeling](https://arxiv.org/pdf/2405.17247v1)** | 2024-05-28 |  |
| **[Uncertainty-Aware Evaluation for Vision-Language Models](https://arxiv.org/pdf/2402.14418v2)** | 2024-02-27 |  |
| **[Evaluating Attribute Comprehension in Large Vision-Language Models](https://arxiv.org/pdf/2408.13898v1)** | 2024-08-27 | 15 pages, 4 figures |
| **[On the Domain Robustness of Contrastive Vision-Language Models](https://arxiv.org/pdf/2506.23663v1)** | 2025-07-01 | <details><summary>Deepb...</summary><p>Deepbench is available at https://github.com/ml-lab-htw/deepbench</p></details> |
| **[HoVLE: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding](https://arxiv.org/pdf/2412.16158v2)** | 2025-02-11 |  |
| **[A Review of 3D Object Detection with Vision-Language Models](https://arxiv.org/pdf/2504.18738v1)** | 2025-04-29 |  |
| **[Vision-Language Foundation Models as Effective Robot Imitators](https://arxiv.org/pdf/2311.01378v3)** | 2024-02-06 | <details><summary>Fix t...</summary><p>Fix typos. Project page: https://roboflamingo.github.io</p></details> |
| **[CoLLaVO: Crayon Large Language and Vision mOdel](https://arxiv.org/pdf/2402.11248v4)** | 2024-06-04 | <details><summary>ACL 2...</summary><p>ACL 2024 Findings. Code available: https://github.com/ByungKwanLee/CoLLaVO</p></details> |
| **[Unified Vision-Language-Action Model](https://arxiv.org/pdf/2506.19850v1)** | 2025-06-25 | technical report |

## world model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[World-in-World: World Models in a Closed-Loop World](https://arxiv.org/pdf/2510.18135v1)** | 2025-10-22 | <details><summary>Code ...</summary><p>Code is at https://github.com/World-In-World/world-in-world</p></details> |
| **[RLVR-World: Training World Models with Reinforcement Learning](https://arxiv.org/pdf/2505.13934v2)** | 2025-10-28 | <details><summary>NeurI...</summary><p>NeurIPS 2025. Code is available at project website: https://thuml.github.io/RLVR-World/</p></details> |
| **[World Models](https://arxiv.org/pdf/1803.10122v4)** | 2018-05-10 |  |
| **[Critiques of World Models](https://arxiv.org/pdf/2507.05169v3)** | 2025-07-29 |  |
| **[Semantic World Models](https://arxiv.org/pdf/2510.19818v1)** | 2025-10-23 |  |
| **[PoE-World: Compositional World Modeling with Products of Programmatic Experts](https://arxiv.org/pdf/2505.10819v3)** | 2025-10-17 |  |
| **[From Masks to Worlds: A Hitchhiker's Guide to World Models](https://arxiv.org/pdf/2510.20668v1)** | 2025-10-24 | <details><summary>Githu...</summary><p>Github: https://github.com/M-E-AGI-Lab/Awesome-World-Models</p></details> |
| **[Predictive World Models from Real-World Partial Observations](https://arxiv.org/pdf/2301.04783v3)** | 2023-05-24 | <details><summary>Best ...</summary><p>Best Paper Award at IEEE MOST 2023</p></details> |
| **[Quantifying Multimodality in World Models](https://arxiv.org/pdf/2112.07263v1)** | 2021-12-15 |  |
| **[Clone Deterministic 3D Worlds with Geometrically-Regularized World Models](https://arxiv.org/pdf/2510.26782v1)** | 2025-10-31 |  |
| **[Understanding World or Predicting Future? A Comprehensive Survey of World Models](https://arxiv.org/pdf/2411.14499v2)** | 2025-06-26 | <details><summary>Accep...</summary><p>Accepted by ACM CSUR, 37 pages, 7 figures, 7 tables</p></details> |
| **[Finetuning Offline World Models in the Real World](https://arxiv.org/pdf/2310.16029v1)** | 2023-10-25 | <details><summary>CoRL ...</summary><p>CoRL 2023 Oral; Project website: https://yunhaifeng.com/FOWM</p></details> |
| **[Modeling Worlds in Text](https://arxiv.org/pdf/2106.09578v1)** | 2021-06-18 | <details><summary>Prepr...</summary><p>Preprint. Under review. Benchmark can be found at https://github.com/JerichoWorld/JerichoWorld</p></details> |
| **[Toward Stable World Models: Measuring and Addressing World Instability in Generative Environments](https://arxiv.org/pdf/2503.08122v1)** | 2025-03-12 | Preprint |
| **[A General World Model with Poiesis: Poppers Three Worlds updated with Software](https://arxiv.org/pdf/1604.00360v1)** | 2016-04-04 | <details><summary>9 pag...</summary><p>9 pages. 1 Figure of a world model with physics, software and Geist, Book published 2016 (in German) by Springer, Heidelberg, with the general fundamentals of Software for philosophy</p></details> |

