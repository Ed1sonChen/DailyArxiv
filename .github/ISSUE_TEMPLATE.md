---
title: Latest 15 Papers - February 04, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## MLLM
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Oryx MLLM: On-Demand Spatial-Temporal Understanding at Arbitrary Resolution](http://arxiv.org/abs/2409.12961v3)** | 2025-01-31 | <details><summary>Accep...</summary><p>Accepted to ICLR 2025</p></details> |
| **[Beyond Token Compression: A Training-Free Reduction Framework for Efficient Visual Processing in MLLMs](http://arxiv.org/abs/2501.19036v1)** | 2025-01-31 |  |
| **[InternVideo2.5: Empowering Video MLLMs with Long and Rich Context Modeling](http://arxiv.org/abs/2501.12386v2)** | 2025-01-22 | technical report |
| **[Beyond Specialization: Assessing the Capabilities of MLLMs in Age and Gender Estimation](http://arxiv.org/abs/2403.02302v4)** | 2025-01-21 |  |
| **[Redundancy Principles for MLLMs Benchmarks](http://arxiv.org/abs/2501.13953v1)** | 2025-01-20 |  |
| **[Visual RAG: Expanding MLLM visual knowledge without fine-tuning](http://arxiv.org/abs/2501.10834v1)** | 2025-01-18 |  |
| **[Omni-Emotion: Extending Video MLLM with Detailed Face and Audio Modeling for Multimodal Emotion Analysis](http://arxiv.org/abs/2501.09502v1)** | 2025-01-16 |  |
| **[Compression with Global Guidance: Towards Training-free High-Resolution MLLMs Acceleration](http://arxiv.org/abs/2501.05179v2)** | 2025-01-15 | <details><summary>Our c...</summary><p>Our code is released at \url{https://github.com/xuyang-liu16/GlobalCom2}</p></details> |
| **[An empirical study of LLaMA3 quantization: from LLMs to MLLMs](http://arxiv.org/abs/2404.14047v3)** | 2025-01-13 |  |
| **[Open Eyes, Then Reason: Fine-grained Visual Mathematical Understanding in MLLMs](http://arxiv.org/abs/2501.06430v1)** | 2025-01-11 |  |
| **[PEACE: Empowering Geologic Map Holistic Understanding with MLLMs](http://arxiv.org/abs/2501.06184v1)** | 2025-01-10 |  |
| **[Image-based Multimodal Models as Intruders: Transferable Multimodal Attacks on Video-based MLLMs](http://arxiv.org/abs/2501.01042v2)** | 2025-01-10 |  |
| **[Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal ReAsoning Benchmark](http://arxiv.org/abs/2501.05444v1)** | 2025-01-09 |  |
| **[Benchmarking Large and Small MLLMs](http://arxiv.org/abs/2501.04150v1)** | 2025-01-04 |  |
| **[Virgo: A Preliminary Exploration on Reproducing o1-like MLLM](http://arxiv.org/abs/2501.01904v1)** | 2025-01-03 | <details><summary>Techn...</summary><p>Technical Report on Slow Thinking with LLMs: Visual Reasoning</p></details> |

## Vision Language Action
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[An Atomic Skill Library Construction Method for Data-Efficient Embodied Manipulation](http://arxiv.org/abs/2501.15068v2)** | 2025-02-03 |  |
| **[HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers](http://arxiv.org/abs/2410.05273v3)** | 2025-02-03 | <details><summary>Accep...</summary><p>Accepted to CORL 2024</p></details> |
| **[UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent](http://arxiv.org/abs/2501.18867v2)** | 2025-02-03 |  |
| **[LLaRA: Supercharging Robot Learning Data for Vision-Language Policy](http://arxiv.org/abs/2406.20095v3)** | 2025-01-30 | ICLR 2025 |
| **[Improving Vision-Language-Action Model with Online Reinforcement Learning](http://arxiv.org/abs/2501.16664v1)** | 2025-01-28 | <details><summary>Accep...</summary><p>Accepted to ICRA 2025</p></details> |
| **[GeoManip: Geometric Constraints as General Interfaces for Robot Manipulation](http://arxiv.org/abs/2501.09783v1)** | 2025-01-16 | 32 pages, 13 figures |
| **[FAST: Efficient Action Tokenization for Vision-Language-Action Models](http://arxiv.org/abs/2501.09747v1)** | 2025-01-16 | <details><summary>Websi...</summary><p>Website: https://www.pi.website/research/fast</p></details> |
| **[Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous Sensors via Language Grounding](http://arxiv.org/abs/2501.04693v3)** | 2025-01-14 |  |
| **[Shake-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Manipulations and Liquid Mixing](http://arxiv.org/abs/2501.06919v1)** | 2025-01-12 | <details><summary>Accep...</summary><p>Accepted to IEEE/ACM HRI 2025</p></details> |
| **[UAV-VLA: Vision-Language-Action System for Large Scale Aerial Mission Generation](http://arxiv.org/abs/2501.05014v1)** | 2025-01-09 | HRI 2025 |
| **[OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints](http://arxiv.org/abs/2501.03841v1)** | 2025-01-07 |  |
| **[Large language models for artificial general intelligence (AGI): A survey of foundational principles and approaches](http://arxiv.org/abs/2501.03151v1)** | 2025-01-06 |  |
| **[Improving Vision-Language-Action Models via Chain-of-Affordance](http://arxiv.org/abs/2412.20451v1)** | 2024-12-29 | <details><summary>Proje...</summary><p>Project webpage is available at https://chain-of-affordance.github.io</p></details> |
| **[TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies](http://arxiv.org/abs/2412.10345v2)** | 2024-12-25 |  |
| **[VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks](http://arxiv.org/abs/2412.18194v1)** | 2024-12-24 |  |

