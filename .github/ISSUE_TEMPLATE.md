---
title: Latest 15 Papers - September 15, 2025
labels: documentation
---
**Please check the [Github](https://github.com/Ed1sonChen/DailyArxiv) page for a better reading experience and more papers.**

## Vision Language Action
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning](http://arxiv.org/abs/2509.09674v1)** | 2025-09-11 |  |
| **[VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model](http://arxiv.org/abs/2509.09372v1)** | 2025-09-11 |  |
| **[villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models](http://arxiv.org/abs/2507.23682v2)** | 2025-09-11 | <details><summary>Proje...</summary><p>Project page: https://aka.ms/villa-x</p></details> |
| **[SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models](http://arxiv.org/abs/2509.09090v1)** | 2025-09-11 | 12 pages, 9 figures |
| **[RoboChemist: Long-Horizon and Safety-Compliant Robotic Chemical Experimentation](http://arxiv.org/abs/2509.08820v1)** | 2025-09-10 | <details><summary>Accep...</summary><p>Accepted to CoRL 2025, Project Page: https://zzongzheng0918.github.io/RoboChemist.github.io/</p></details> |
| **[LLaDA-VLA: Vision Language Diffusion Action Models](http://arxiv.org/abs/2509.06932v2)** | 2025-09-10 |  |
| **[TA-VLA: Elucidating the Design Space of Torque-aware Vision-Language-Action Models](http://arxiv.org/abs/2509.07962v1)** | 2025-09-09 | <details><summary>Accep...</summary><p>Accepted to CoRL 2025, project page: \url{https://zzongzheng0918.github.io/Torque-Aware-VLA.github.io/}</p></details> |
| **[Graph-Fused Vision-Language-Action for Policy Reasoning in Multi-Arm Robotic Manipulation](http://arxiv.org/abs/2509.07957v1)** | 2025-09-09 | <details><summary>This ...</summary><p>This paper is submitted to IEEE IROS 2025 Workshop AIR4S</p></details> |
| **[EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control](http://arxiv.org/abs/2508.21112v3)** | 2025-09-09 |  |
| **[F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions](http://arxiv.org/abs/2509.06951v2)** | 2025-09-09 | <details><summary>Homep...</summary><p>Homepage: https://aopolin-lv.github.io/F1-VLA/</p></details> |
| **[CRISP -- Compliant ROS2 Controllers for Learning-Based Manipulation Policies and Teleoperation](http://arxiv.org/abs/2509.06819v1)** | 2025-09-08 | 5 pages, 5 figures |
| **[Dita: Scaling Diffusion Transformer for Generalist Vision-Language-Action Policy](http://arxiv.org/abs/2503.19757v2)** | 2025-09-06 | <details><summary>Prepr...</summary><p>Preprint; https://robodita.github.io; To appear in ICCV2025</p></details> |
| **[4D Visual Pre-training for Robot Learning](http://arxiv.org/abs/2508.17230v2)** | 2025-09-06 |  |
| **[SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning](http://arxiv.org/abs/2509.05614v1)** | 2025-09-06 | 8pages, 10 figures, |
| **[OccVLA: Vision-Language-Action Model with Implicit 3D Occupancy Supervision](http://arxiv.org/abs/2509.05578v1)** | 2025-09-06 |  |

## robot
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Coordinated Motion Planning of a Wearable Multi-Limb System for Enhanced Human-Robot Interaction](http://arxiv.org/abs/2509.10444v1)** | 2025-09-12 | <details><summary>Prese...</summary><p>Presented in IROS 2023 Workshop (Multilimb Coordination in Human Neuroscience and Robotics: Classical and Learning Perspectives)</p></details> |
| **[Repeatable Energy-Efficient Perching for Flapping-Wing Robots Using Soft Grippers](http://arxiv.org/abs/2409.11921v2)** | 2025-09-12 | <details><summary>16 pa...</summary><p>16 pages, 16 figures, 5 multimedia extensions</p></details> |
| **[Data-fused Model Predictive Control with Guarantees: Application to Flying Humanoid Robots](http://arxiv.org/abs/2509.10353v1)** | 2025-09-12 | 8 pages, 3 figures |
| **[Robot guide with multi-agent control and automatic scenario generation with LLM](http://arxiv.org/abs/2509.10317v1)** | 2025-09-12 | <details><summary>14 pa...</summary><p>14 pages, 5 figures, 2 tables, 1 demo-video and repository link</p></details> |
| **[GundamQ: Multi-Scale Spatio-Temporal Representation Learning for Robust Robot Path Planning](http://arxiv.org/abs/2509.10305v1)** | 2025-09-12 | 6 pages, 5 figures |
| **[Environmental force sensing helps robots traverse cluttered large obstacles using physical interaction](http://arxiv.org/abs/2112.07900v3)** | 2025-09-12 |  |
| **[MiniTac: An Ultra-Compact 8 mm Vision-Based Tactile Sensor for Enhanced Palpation in Robot-Assisted Minimally Invasive Surgery](http://arxiv.org/abs/2410.22691v2)** | 2025-09-12 | <details><summary>accep...</summary><p>accepted for publication in the IEEE Robotics and Automation Letters (RA-L)</p></details> |
| **[Embedding high-resolution touch across robotic hands enables adaptive human-like grasping](http://arxiv.org/abs/2412.14482v3)** | 2025-09-12 |  |
| **[Efficient Learning-Based Control of a Legged Robot in Lunar Gravity](http://arxiv.org/abs/2509.10128v1)** | 2025-09-12 |  |
| **[Taccel: Scaling Up Vision-based Tactile Robotics via High-performance GPU Simulation](http://arxiv.org/abs/2504.12908v2)** | 2025-09-12 |  |
| **[CTBC: Contact-Triggered Blind Climbing for Wheeled Bipedal Robots with Instruction Learning and Reinforcement Learning](http://arxiv.org/abs/2509.02986v2)** | 2025-09-12 |  |
| **[RGBlimp-Q: Robotic Gliding Blimp With Moving Mass Control Based on a Bird-Inspired Continuum Arm](http://arxiv.org/abs/2406.10810v2)** | 2025-09-12 |  |
| **[Gaussian path model library for intuitive robot motion programming by demonstration](http://arxiv.org/abs/2509.10007v1)** | 2025-09-12 |  |
| **[Collision-Inclusive Manipulation Planning for Occluded Object Grasping via Compliant Robot Motions](http://arxiv.org/abs/2412.06983v2)** | 2025-09-12 | <details><summary>This ...</summary><p>This work has been submitted to the IEEE for possible publication</p></details> |
| **[Object-Centric Kinodynamic Planning for Nonprehensile Robot Rearrangement Manipulation](http://arxiv.org/abs/2410.00261v3)** | 2025-09-12 |  |

## Vision Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse](http://arxiv.org/abs/2503.16365v2)** | 2025-09-12 | Accepted by ACL 2025 |
| **[TASC: Task-Aware Shared Control for Teleoperated Manipulation](http://arxiv.org/abs/2509.10416v1)** | 2025-09-12 |  |
| **[Towards Understanding Visual Grounding in Visual Language Models](http://arxiv.org/abs/2509.10345v1)** | 2025-09-12 |  |
| **[Talk2PC: Enhancing 3D Visual Grounding through LiDAR and Radar Point Clouds Fusion for Autonomous Driving](http://arxiv.org/abs/2503.08336v2)** | 2025-09-12 | 13 pages, 12 figures |
| **[Detecting Text Manipulation in Images using Vision Language Models](http://arxiv.org/abs/2509.10278v1)** | 2025-09-12 | <details><summary>Accep...</summary><p>Accepted in Synthetic Realities and Biometric Security Workshop BMVC-2025. For paper page see https://www.idiap.ch/paper/textvlmdet/</p></details> |
| **[MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation](http://arxiv.org/abs/2509.10260v1)** | 2025-09-12 |  |
| **[Towards Reliable and Interpretable Document Question Answering via VLMs](http://arxiv.org/abs/2509.10129v1)** | 2025-09-12 |  |
| **[VARCO-VISION-2.0 Technical Report](http://arxiv.org/abs/2509.10105v1)** | 2025-09-12 | <details><summary>19 pa...</summary><p>19 pages, 1 figure, 14 tables. Technical report for VARCO-VISION-2.0, a Korean-English bilingual VLM in 14B and 1.7B variants. Key features: multi-image understanding, OCR with text localization, improved Korean capabilities</p></details> |
| **[GROVE: A Generalized Reward for Learning Open-Vocabulary Physical Skill](http://arxiv.org/abs/2504.04191v2)** | 2025-09-12 |  |
| **[MedM-VL: What Makes a Good Medical LVLM?](http://arxiv.org/abs/2504.04323v3)** | 2025-09-12 |  |
| **[When and How Does CLIP Enable Domain and Compositional Generalization?](http://arxiv.org/abs/2502.09507v3)** | 2025-09-12 | <details><summary>ICML ...</summary><p>ICML 2025 (Spotlight)</p></details> |
| **[Multimodal Mathematical Reasoning Embedded in Aerial Vehicle Imagery: Benchmarking, Analysis, and Exploration](http://arxiv.org/abs/2509.10059v1)** | 2025-09-12 | 17 pages, 16 figures |
| **[LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA](http://arxiv.org/abs/2509.10026v1)** | 2025-09-12 | <details><summary>12 Pa...</summary><p>12 Pages, 12 Figures, 2 Tables</p></details> |
| **[MoPD: Mixture-of-Prompts Distillation for Vision-Language Models](http://arxiv.org/abs/2412.19087v2)** | 2025-09-12 |  |
| **[Self-Rewarding Large Vision-Language Models for Optimizing Prompts in Text-to-Image Generation](http://arxiv.org/abs/2505.16763v2)** | 2025-09-12 | <details><summary>Accep...</summary><p>Accepted by ACL2025 Findings</p></details> |

## world model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Slaves to the Law of Large Numbers: An Asymptotic Equipartition Property for Perplexity in Generative Language Models](http://arxiv.org/abs/2405.13798v4)** | 2025-09-12 |  |
| **[LaDi-WM: A Latent Diffusion-based World Model for Predictive Manipulation](http://arxiv.org/abs/2505.11528v6)** | 2025-09-12 | CoRL 2025 |
| **[3D and 4D World Modeling: A Survey](http://arxiv.org/abs/2509.07996v2)** | 2025-09-11 | <details><summary>Surve...</summary><p>Survey; 34 pages, 10 figures, 14 tables; GitHub Repo at https://github.com/worldbench/survey</p></details> |
| **[World Modeling with Probabilistic Structure Integration](http://arxiv.org/abs/2509.09737v1)** | 2025-09-10 |  |
| **[Randomly Sampled Language Reasoning Problems Elucidate Limitations of In-Context Learning](http://arxiv.org/abs/2501.02825v6)** | 2025-09-10 | <details><summary>10 pa...</summary><p>10 pages, 4 figures, 2 tables</p></details> |
| **[A Survey of World Models for Autonomous Driving](http://arxiv.org/abs/2501.11260v4)** | 2025-09-10 | <details><summary>Ongoi...</summary><p>Ongoing project. Paper list: https://github.com/FengZicai/AwesomeWMAD Benchmark: https://github.com/FengZicai/WMAD-Benchmarks</p></details> |
| **[One Model for All Tasks: Leveraging Efficient World Models in Multi-Task Planning](http://arxiv.org/abs/2509.07945v1)** | 2025-09-09 | 43 pages, 19 figures |
| **[LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences](http://arxiv.org/abs/2508.03692v2)** | 2025-09-09 | <details><summary>Prepr...</summary><p>Preprint; 28 pages, 18 figures, 12 tables; Project Page at https://lidarcrafter.github.io</p></details> |
| **[When Do Neural Networks Learn World Models?](http://arxiv.org/abs/2502.09297v5)** | 2025-09-09 | <details><summary>ICML ...</summary><p>ICML 2025; ICLR 2025 World Models Workshop (oral, outstanding paper award)</p></details> |
| **[Semi-SMD: Semi-Supervised Metric Depth Estimation via Surrounding Cameras for Autonomous Driving](http://arxiv.org/abs/2503.19713v3)** | 2025-09-09 |  |

