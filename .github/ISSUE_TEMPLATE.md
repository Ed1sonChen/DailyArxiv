---
title: Latest 15 Papers - February 08, 2026
labels: documentation
---
**Please check the [Github](https://github.com/Ed1sonChen/DailyArxiv) page for a better reading experience and more papers.**

## Vision Language Action
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism](https://arxiv.org/abs/2602.05765v1)** | 2026-02-05 |  |
| **[Benchmarking Affordance Generalization with BusyBox](https://arxiv.org/abs/2602.05441v1)** | 2026-02-05 |  |
| **[RoboPaint: From Human Demonstration to Any Robot and Any View](https://arxiv.org/abs/2602.05325v1)** | 2026-02-05 | 17 pages |
| **[MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning](https://arxiv.org/abs/2512.13636v3)** | 2026-02-05 | <details><summary>16 pa...</summary><p>16 pages, 12 figures, 6 tables; Project Page: https://xiaomi-mlab.github.io/MindDrive/</p></details> |
| **[MobileManiBench: Simplifying Model Verification for Mobile Manipulation](https://arxiv.org/abs/2602.05233v1)** | 2026-02-05 |  |
| **[VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models](https://arxiv.org/abs/2602.05049v1)** | 2026-02-04 | <details><summary>In su...</summary><p>In submission. Project website: https://vista-vla.github.io/</p></details> |
| **[Open-Source Multimodal Moxin Models with Moxin-VLM and Moxin-VLA](https://arxiv.org/abs/2512.22208v2)** | 2026-02-04 |  |
| **[Act, Sense, Act: Learning Non-Markovian Active Perception Strategies from Large-Scale Egocentric Human Data](https://arxiv.org/abs/2602.04600v1)** | 2026-02-04 |  |
| **[PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence](https://arxiv.org/abs/2512.16793v2)** | 2026-02-04 | 21 pages, 8 figures |
| **[GeneralVLA: Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning](https://arxiv.org/abs/2602.04315v1)** | 2026-02-04 |  |
| **[A Survey on Vision-Language-Action Models for Embodied AI](https://arxiv.org/abs/2405.14093v7)** | 2026-02-04 | <details><summary>Proje...</summary><p>Project page: https://github.com/yueen-ma/Awesome-VLA</p></details> |
| **[Reshaping Action Error Distributions for Reliable Vision-Language-Action Models](https://arxiv.org/abs/2602.04228v1)** | 2026-02-04 |  |
| **[SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models](https://arxiv.org/abs/2602.04208v1)** | 2026-02-04 | 20 pages, 8 figures |
| **[Natural Language Instructions for Scene-Responsive Human-in-the-Loop Motion Planning in Autonomous Driving using Vision-Language-Action Models](https://arxiv.org/abs/2602.04184v1)** | 2026-02-04 |  |
| **[Efficient Long-Horizon Vision-Language-Action Models via Static-Dynamic Disentanglement](https://arxiv.org/abs/2602.03983v1)** | 2026-02-03 |  |

## robot
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Physical Human-Robot Interaction: A Critical Review of Safety Constraints](https://arxiv.org/abs/2601.19462v2)** | 2026-02-05 |  |
| **[A Hybrid Autoencoder for Robust Heightmap Generation from Fused Lidar and Depth Data for Humanoid Robot Locomotion](https://arxiv.org/abs/2602.05855v1)** | 2026-02-05 |  |
| **[Task-Oriented Robot-Human Handovers on Legged Manipulators](https://arxiv.org/abs/2602.05760v1)** | 2026-02-05 | <details><summary>Accep...</summary><p>Accepted to 21st ACM/IEEE International Conference on Human-Robot Interaction (HRI) 2026</p></details> |
| **[Dull, Dirty, Dangerous: Understanding the Past, Present, and Future of a Key Motivation for Robotics](https://arxiv.org/abs/2602.04746v2)** | 2026-02-05 |  |
| **[Constraint-Aware Discrete-Time PID Gain Optimization for Robotic Joint Control Under Actuator Saturation](https://arxiv.org/abs/2601.18639v3)** | 2026-02-05 | <details><summary>Pendi...</summary><p>Pending IEEE Transactions on Robotics Publication</p></details> |
| **[The Use of AI-Robotic Systems for Scientific Discovery](https://arxiv.org/abs/2406.17835v2)** | 2026-02-05 | <details><summary>23 pa...</summary><p>23 pages, book chapter</p></details> |
| **[PIRATR: Parametric Object Inference for Robotic Applications with Transformers in 3D Point Clouds](https://arxiv.org/abs/2602.05557v1)** | 2026-02-05 | <details><summary>8 Pag...</summary><p>8 Pages, 11 Figures, Accepted at 2026 IEEE International Conference on Robotics & Automation (ICRA) Vienna</p></details> |
| **[FilMBot: A High-Speed Soft Parallel Robotic Micromanipulator](https://arxiv.org/abs/2410.23059v3)** | 2026-02-05 | 13 pages, 16 figures |
| **[Do Robots Really Need Anthropomorphic Hands? -- A Comparison of Human and Robotic Hands](https://arxiv.org/abs/2508.05415v2)** | 2026-02-05 |  |
| **[Ontology-Driven Robotic Specification Synthesis](https://arxiv.org/abs/2602.05456v1)** | 2026-02-05 | <details><summary>8 pag...</summary><p>8 pages, 9 figures, 3 tables, journal</p></details> |
| **[RoboPaint: From Human Demonstration to Any Robot and Any View](https://arxiv.org/abs/2602.05325v1)** | 2026-02-05 | 17 pages |
| **[Learning Soccer Skills for Humanoid Robots: A Progressive Perception-Action Framework](https://arxiv.org/abs/2602.05310v1)** | 2026-02-05 | <details><summary>13 pa...</summary><p>13 pages, 9 figures, conference</p></details> |
| **[Low-Cost Underwater In-Pipe Centering and Inspection Using a Minimal-Sensing Robot](https://arxiv.org/abs/2602.05265v1)** | 2026-02-05 |  |
| **[Trojan Attacks on Neural Network Controllers for Robotic Systems](https://arxiv.org/abs/2602.05121v1)** | 2026-02-04 | <details><summary>Paper...</summary><p>Paper submitted to the 2026 IEEE Conference on Control Technology and Applications (CCTA)</p></details> |
| **[Differentiable Inverse Graphics for Zero-shot Scene Reconstruction and Robot Grasping](https://arxiv.org/abs/2602.05029v1)** | 2026-02-04 | <details><summary>Submi...</summary><p>Submitted to IEEE Robotics and Automation Letters (RA-L) for review. This version includes the statement required by IEEE for preprints</p></details> |

## Vision Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Can vision language models learn intuitive physics from interaction?](https://arxiv.org/abs/2602.06033v1)** | 2026-02-05 |  |
| **[GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?](https://arxiv.org/abs/2602.06013v1)** | 2026-02-05 | <details><summary>Proje...</summary><p>Project Page: https://genarena.github.io/, Code: https://github.com/ruihanglix/genarena</p></details> |
| **[GIQ: Benchmarking 3D Geometric Reasoning of Vision Foundation Models with Simulated and Real Polyhedra](https://arxiv.org/abs/2506.08194v3)** | 2026-02-05 | <details><summary>Accep...</summary><p>Accepted to ICLR 2026. Camera ready version</p></details> |
| **[Focus-Scan-Refine: From Human Visual Perception to Efficient Visual Token Pruning](https://arxiv.org/abs/2602.05809v1)** | 2026-02-05 |  |
| **[Allocentric Perceiver: Disentangling Allocentric Reasoning from Egocentric Visual Priors via Frame Instantiation](https://arxiv.org/abs/2602.05789v1)** | 2026-02-05 |  |
| **[Ethology of Latent Spaces](https://arxiv.org/abs/2602.05710v1)** | 2026-02-05 | <details><summary>23. p...</summary><p>23. pages, 14 figures, presented Hyperheritage International Symposium 9 ( https://paragraphe.univ-paris8.fr/IMG/pdf/programme_colloque_his9_campuscondorcet_v3.pdf ) and accepted for publication in double-blind peer review in French in 2026-2027</p></details> |
| **[LoGoSeg: Integrating Local and Global Features for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2602.05578v1)** | 2026-02-05 |  |
| **[PIO-FVLM: Rethinking Training-Free Visual Token Reduction for VLM Acceleration from an Inference-Objective Perspective](https://arxiv.org/abs/2602.04657v2)** | 2026-02-05 |  |
| **[TangramSR: Can Vision-Language Models Reason in Continuous Geometric Space?](https://arxiv.org/abs/2602.05570v1)** | 2026-02-05 | 13 pages, 4 figures |
| **[VLN-Pilot: Large Vision-Language Model as an Autonomous Indoor Drone Operator](https://arxiv.org/abs/2602.05552v1)** | 2026-02-05 |  |
| **[Detecting Misbehaviors of Large Vision-Language Models by Evidential Uncertainty Quantification](https://arxiv.org/abs/2602.05535v1)** | 2026-02-05 | <details><summary>Accep...</summary><p>Accepted to ICLR 2026. Code is available at https://github.com/HT86159/EUQ</p></details> |
| **[RANGER: A Monocular Zero-Shot Semantic Navigation Framework through Contextual Adaptation](https://arxiv.org/abs/2512.24212v2)** | 2026-02-05 | <details><summary>Accep...</summary><p>Accepted at ICRA 2026</p></details> |
| **[See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning](https://arxiv.org/abs/2512.22120v2)** | 2026-02-05 |  |
| **[Once Correct, Still Wrong: Counterfactual Hallucination in Multilingual Vision-Language Models](https://arxiv.org/abs/2602.05437v1)** | 2026-02-05 |  |
| **[Dolphin-v2: Universal Document Parsing via Scalable Anchor Prompting](https://arxiv.org/abs/2602.05384v1)** | 2026-02-05 |  |

## world model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Visuo-Tactile World Models](https://arxiv.org/abs/2602.06001v1)** | 2026-02-05 | Preprint |
| **[Verification of the Implicit World Model in a Generative Model via Adversarial Sequences](https://arxiv.org/abs/2602.05903v1)** | 2026-02-05 | <details><summary>Accep...</summary><p>Accepted at ICLR 2026. Code, datasets, and models are available at https://github.com/szegedai/world-model-verification</p></details> |
| **[The Double Life of Code World Models: Provably Unmasking Malicious Behavior Through Execution Traces](https://arxiv.org/abs/2512.13821v2)** | 2026-02-05 | 13 Pages, A Preprint |
| **[Reinforcement World Model Learning for LLM-based Agents](https://arxiv.org/abs/2602.05842v1)** | 2026-02-05 |  |
| **[PIRATR: Parametric Object Inference for Robotic Applications with Transformers in 3D Point Clouds](https://arxiv.org/abs/2602.05557v1)** | 2026-02-05 | <details><summary>8 Pag...</summary><p>8 Pages, 11 Figures, Accepted at 2026 IEEE International Conference on Robotics & Automation (ICRA) Vienna</p></details> |
| **[Safe Urban Traffic Control via Uncertainty-Aware Conformal Prediction and World-Model Reinforcement Learning](https://arxiv.org/abs/2602.04821v1)** | 2026-02-04 |  |
| **[Dual Mind World Model Inspired Network Digital Twin for Access Scheduling](https://arxiv.org/abs/2602.04566v1)** | 2026-02-04 |  |
| **[Language Models Struggle to Use Representations Learned In-Context](https://arxiv.org/abs/2602.04212v1)** | 2026-02-04 |  |
| **[Simulating the Visual World with Artificial Intelligence: A Roadmap](https://arxiv.org/abs/2511.08585v3)** | 2026-02-03 | <details><summary>Proje...</summary><p>Project page: https://world-model-roadmap.github.io/ Github Repo: https://github.com/ziqihuangg/Awesome-From-Video-Generation-to-World-Model</p></details> |
| **[Active Epistemic Control for Query-Efficient Verified Planning](https://arxiv.org/abs/2602.03974v1)** | 2026-02-03 |  |
| **[BridgeV2W: Bridging Video Generation Models to Embodied World Models via Embodiment Masks](https://arxiv.org/abs/2602.03793v1)** | 2026-02-03 |  |
| **[LIVE: Long-horizon Interactive Video World Modeling](https://arxiv.org/abs/2602.03747v1)** | 2026-02-03 | 18 pages, 22 figures |
| **[Infinite-World: Scaling Interactive World Models to 1000-Frame Horizons via Pose-Free Hierarchical Memory](https://arxiv.org/abs/2602.02393v2)** | 2026-02-03 | <details><summary>proje...</summary><p>project page: https://rq-wu.github.io/projects/infinite-world/index.html</p></details> |
| **[A Lightweight Library for Energy-Based Joint-Embedding Predictive Architectures](https://arxiv.org/abs/2602.03604v1)** | 2026-02-03 |  |
| **[Building spatial world models from sparse transitional episodic memories](https://arxiv.org/abs/2505.13696v2)** | 2026-02-03 | Accepted ICLR 2026 |

