---
title: Latest 15 Papers - September 21, 2025
labels: documentation
---
**Please check the [Github](https://github.com/Ed1sonChen/DailyArxiv) page for a better reading experience and more papers.**

## Vision Language Action
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation](http://arxiv.org/abs/2509.15212v1)** | 2025-09-18 | <details><summary>GitHu...</summary><p>GitHub Project: https://github.com/alibaba-damo-academy/RynnVLA-001</p></details> |
| **[Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models](http://arxiv.org/abs/2409.13174v3)** | 2025-09-18 |  |
| **[ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning](http://arxiv.org/abs/2507.16815v2)** | 2025-09-18 | <details><summary>NeurI...</summary><p>NeurIPS 2025. Project page: https://jasper0314-huang.github.io/thinkact-vla/</p></details> |
| **[ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation](http://arxiv.org/abs/2505.22159v3)** | 2025-09-18 | NeurIPS 2025 |
| **[Robot Control Stack: A Lean Ecosystem for Robot Learning at Scale](http://arxiv.org/abs/2509.14932v1)** | 2025-09-18 |  |
| **[CollabVLA: Self-Reflective Vision-Language-Action Model Dreaming Together with Human](http://arxiv.org/abs/2509.14889v1)** | 2025-09-18 | <details><summary>8 pag...</summary><p>8 pages, 5 figures, 3 tables</p></details> |
| **[RealMirror: A Comprehensive, Open-Source Vision-Language-Action Platform for Embodied AI](http://arxiv.org/abs/2509.14687v1)** | 2025-09-18 |  |
| **[Toward Embodiment Equivariant Vision-Language-Action Policy](http://arxiv.org/abs/2509.14630v1)** | 2025-09-18 |  |
| **[CLAW: A Vision-Language-Action Framework for Weight-Aware Robotic Grasping](http://arxiv.org/abs/2509.14143v1)** | 2025-09-17 | <details><summary>8 pag...</summary><p>8 pages, 5 figures, 1 table</p></details> |
| **[SeqVLA: Sequential Task Execution for Long-Horizon Manipulation with Completion-Aware Vision-Language-Action Model](http://arxiv.org/abs/2509.14138v1)** | 2025-09-17 | <details><summary>8 pag...</summary><p>8 pages, 9 figures, 1 table</p></details> |
| **[GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model](http://arxiv.org/abs/2509.14117v1)** | 2025-09-17 | Under Review |
| **[Dual-Actor Fine-Tuning of VLA Models: A Talk-and-Tweak Human-in-the-Loop Approach](http://arxiv.org/abs/2509.13774v1)** | 2025-09-17 |  |
| **[AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving](http://arxiv.org/abs/2509.13769v1)** | 2025-09-17 |  |
| **[TrajBooster: Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning](http://arxiv.org/abs/2509.11839v2)** | 2025-09-17 |  |
| **[Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations](http://arxiv.org/abs/2509.11417v2)** | 2025-09-17 | <details><summary>Proje...</summary><p>Project Page: https://gen-vla.github.io/</p></details> |

## robot
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation](http://arxiv.org/abs/2509.15212v1)** | 2025-09-18 | <details><summary>GitHu...</summary><p>GitHub Project: https://github.com/alibaba-damo-academy/RynnVLA-001</p></details> |
| **[Parallel Simulation of Contact and Actuation for Soft Growing Robots](http://arxiv.org/abs/2509.15180v1)** | 2025-09-18 | <details><summary>26 pa...</summary><p>26 pages, 9 figures, 1 table. Under review</p></details> |
| **[GAF: Gaussian Action Field as a Dynamic World Model for Robotic Manipulation](http://arxiv.org/abs/2506.14135v3)** | 2025-09-18 | <details><summary>http:...</summary><p>http://chaiying1.github.io/GAF.github.io/project_page/</p></details> |
| **[Online Multi-Robot Coordination and Cooperation with Task Precedence Relationships](http://arxiv.org/abs/2509.15052v1)** | 2025-09-18 | <details><summary>20 pa...</summary><p>20 pages, 19 figures, Accepted to IEEE Transactions on Robotics (TR-O) August 2025</p></details> |
| **[Affordance-Based Disambiguation of Surgical Instructions for Collaborative Robot-Assisted Surgery](http://arxiv.org/abs/2509.14967v1)** | 2025-09-18 | <details><summary>To be...</summary><p>To be presented at the 1st Workshop on Intelligent Cobodied Assistance and Robotic Empowerment (iCARE). 2025 Conference on Robot Learning (CoRL)</p></details> |
| **[RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching](http://arxiv.org/abs/2509.14966v1)** | 2025-09-18 |  |
| **[Multi-CAP: A Multi-Robot Connectivity-Aware Hierarchical Coverage Path Planning Algorithm for Unknown Environments](http://arxiv.org/abs/2509.14941v1)** | 2025-09-18 |  |
| **[Robot Control Stack: A Lean Ecosystem for Robot Learning at Scale](http://arxiv.org/abs/2509.14932v1)** | 2025-09-18 |  |
| **[PERAL: Perception-Aware Motion Control for Passive LiDAR Excitation in Spherical Robots](http://arxiv.org/abs/2509.14915v1)** | 2025-09-18 |  |
| **[T-araVLN: Translator for Agricultural Robotic Agents on Vision-and-Language Navigation](http://arxiv.org/abs/2509.06644v4)** | 2025-09-18 |  |
| **[Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution](http://arxiv.org/abs/2509.14816v1)** | 2025-09-18 |  |
| **[UniTracker: Learning Universal Whole-Body Motion Tracker for Humanoid Robots](http://arxiv.org/abs/2507.07356v3)** | 2025-09-18 | <details><summary>three...</summary><p>three-stage universal motion tracker for humanoid robots</p></details> |
| **[Investigating the Effect of LED Signals and Emotional Displays in Human-Robot Shared Workspaces](http://arxiv.org/abs/2509.14748v1)** | 2025-09-18 | 8 pages |
| **[exUMI: Extensible Robot Teaching System with Action-aware Task-agnostic Tactile Representation](http://arxiv.org/abs/2509.14688v1)** | 2025-09-18 | <details><summary>Accep...</summary><p>Accepted at CoRL 2025</p></details> |
| **[Hierarchical Planning and Scheduling for Reconfigurable Multi-Robot Disassembly Systems under Structural Constraints](http://arxiv.org/abs/2509.14564v1)** | 2025-09-18 | 6 pages, 7 figures |

## Vision Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Calibration-Aware Prompt Learning for Medical Vision-Language Models](http://arxiv.org/abs/2509.15226v1)** | 2025-09-18 | <details><summary>Accep...</summary><p>Accepted in BMVC 2025</p></details> |
| **[ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data](http://arxiv.org/abs/2509.15221v1)** | 2025-09-18 |  |
| **[What's the Best Way to Retrieve Slides? A Comparative Study of Multimodal, Caption-Based, and Hybrid Retrieval Techniques](http://arxiv.org/abs/2509.15211v1)** | 2025-09-18 |  |
| **[MedFact-R1: Towards Factual Medical Reasoning via Pseudo-Label Augmentation](http://arxiv.org/abs/2509.15154v1)** | 2025-09-18 | Tech report |
| **[Forecasting and Visualizing Air Quality from Sky Images with Vision-Language Models](http://arxiv.org/abs/2509.15076v1)** | 2025-09-18 | <details><summary>Publi...</summary><p>Published at ICCVW 2025</p></details> |
| **[Handle Object Navigation as Weighted Traveling Repairman Problem](http://arxiv.org/abs/2503.06937v2)** | 2025-09-18 |  |
| **[QuizRank: Picking Images by Quizzing VLMs](http://arxiv.org/abs/2509.15059v1)** | 2025-09-18 |  |
| **[PRISM: Product Retrieval In Shopping Carts using Hybrid Matching](http://arxiv.org/abs/2509.14985v1)** | 2025-09-18 |  |
| **[EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal Ultrasound Intelligence](http://arxiv.org/abs/2509.14977v1)** | 2025-09-18 |  |
| **[Affordance-Based Disambiguation of Surgical Instructions for Collaborative Robot-Assisted Surgery](http://arxiv.org/abs/2509.14967v1)** | 2025-09-18 | <details><summary>To be...</summary><p>To be presented at the 1st Workshop on Intelligent Cobodied Assistance and Robotic Empowerment (iCARE). 2025 Conference on Robot Learning (CoRL)</p></details> |
| **[VLM-E2E: Enhancing End-to-End Autonomous Driving with Multimodal Driver Attention Fusion](http://arxiv.org/abs/2502.18042v2)** | 2025-09-18 |  |
| **[MARIC: Multi-Agent Reasoning for Image Classification](http://arxiv.org/abs/2509.14860v1)** | 2025-09-18 | Preprint |
| **[V-SEAM: Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models](http://arxiv.org/abs/2509.14837v1)** | 2025-09-18 | EMNLP 2025 Main |
| **[The Art of Saying "Maybe": A Conformal Lens for Uncertainty Benchmarking in VLMs](http://arxiv.org/abs/2509.13379v2)** | 2025-09-18 |  |
| **[Frame Sampling Strategies Matter: A Benchmark for small vision language models](http://arxiv.org/abs/2509.14769v1)** | 2025-09-18 |  |

## world model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[GAF: Gaussian Action Field as a Dynamic World Model for Robotic Manipulation](http://arxiv.org/abs/2506.14135v3)** | 2025-09-18 | <details><summary>http:...</summary><p>http://chaiying1.github.io/GAF.github.io/project_page/</p></details> |
| **[Designing Latent Safety Filters using Pre-Trained Vision Models](http://arxiv.org/abs/2509.14758v1)** | 2025-09-18 |  |
| **[Brain Inspired Probabilistic Occupancy Grid Mapping with Vector Symbolic Architectures](http://arxiv.org/abs/2408.09066v4)** | 2025-09-17 |  |
| **[PhysicalAgent: Towards General Cognitive Robotics with Foundation World Models](http://arxiv.org/abs/2509.13903v1)** | 2025-09-17 | <details><summary>submi...</summary><p>submitted to IEEE conference</p></details> |
| **[DAVIS: Planning Agent with Knowledge Graph-Powered Inner Monologue](http://arxiv.org/abs/2410.09252v2)** | 2025-09-17 | <details><summary>Accep...</summary><p>Accepted to EMNLP 2025 Findings</p></details> |
| **[GWM: Towards Scalable Gaussian World Models for Robotic Manipulation](http://arxiv.org/abs/2508.17600v2)** | 2025-09-17 | <details><summary>Publi...</summary><p>Published at ICCV 2025. Project page: https://gaussian-world-model.github.io/</p></details> |
| **[From Next Token Prediction to (STRIPS) World Models -- Preliminary Results](http://arxiv.org/abs/2509.13389v1)** | 2025-09-16 | 10 pages, 3 figures |
| **[Empowering Multi-Robot Cooperation via Sequential World Models](http://arxiv.org/abs/2509.13095v1)** | 2025-09-16 |  |
| **[A tree-based Polynomial Chaos expansion for surrogate modeling and sensitivity analysis of complex numerical models](http://arxiv.org/abs/2509.13384v1)** | 2025-09-16 |  |
| **[Methodology of Algorithm Engineering](http://arxiv.org/abs/2310.18979v2)** | 2025-09-16 |  |
| **[Enhancing Physical Consistency in Lightweight World Models](http://arxiv.org/abs/2509.12437v1)** | 2025-09-15 | 8 pages |
| **[Causal-Symbolic Meta-Learning (CSML): Inducing Causal World Models for Few-Shot Generalization](http://arxiv.org/abs/2509.12387v1)** | 2025-09-15 | 10 pages, 4 figures |
| **[OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling](http://arxiv.org/abs/2509.12201v1)** | 2025-09-15 | <details><summary>https...</summary><p>https://yangzhou24.github.io/OmniWorld/</p></details> |
| **[Learning to Generate 4D LiDAR Sequences](http://arxiv.org/abs/2509.11959v1)** | 2025-09-15 | <details><summary>Abstr...</summary><p>Abstract Paper (Non-Archival) @ ICCV 2025 Wild3D Workshop; GitHub Repo at https://lidarcrafter.github.io/</p></details> |
| **[Neuro-Symbolic Agents with Modal Logic for Autonomous Diagnostics](http://arxiv.org/abs/2509.11943v1)** | 2025-09-15 | <details><summary>10 pa...</summary><p>10 pages, 1 figure, Scaling Environments for Agents (SEA) Workshop at NeuralIPS</p></details> |

