---
title: Latest 15 Papers - August 27, 2025
labels: documentation
---
**Please check the [Github](https://github.com/Ed1sonChen/DailyArxiv) page for a better reading experience and more papers.**

## Vision Language Action
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation](http://arxiv.org/abs/2508.19236v1)** | 2025-08-26 | <details><summary>The p...</summary><p>The project is available at https://shihao1895.github.io/MemoryVLA</p></details> |
| **[FlowVLA: Thinking in Motion with a Visual Chain of Thought](http://arxiv.org/abs/2508.18269v2)** | 2025-08-26 |  |
| **[DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge](http://arxiv.org/abs/2507.04447v3)** | 2025-08-26 |  |
| **[MapleGrasp: Mask-guided Feature Pooling for Language-driven Efficient Robotic Grasping](http://arxiv.org/abs/2506.06535v3)** | 2025-08-25 |  |
| **[Continual Learning for Generative AI: From LLMs to MLLMs and Beyond](http://arxiv.org/abs/2506.13045v4)** | 2025-08-24 | Preprint |
| **[4D Visual Pre-training for Robot Learning](http://arxiv.org/abs/2508.17230v1)** | 2025-08-24 |  |
| **[GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions](http://arxiv.org/abs/2508.07650v2)** | 2025-08-23 | 10 pages, 6 figures |
| **[NinA: Normalizing Flows in Action. Training VLA Models with Normalizing Flows](http://arxiv.org/abs/2508.16845v1)** | 2025-08-23 |  |
| **[Do What? Teaching Vision-Language-Action Models to Reject the Impossible](http://arxiv.org/abs/2508.16292v1)** | 2025-08-22 | <details><summary>9 pag...</summary><p>9 pages, 2 figures, 1 table</p></details> |
| **[OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing](http://arxiv.org/abs/2508.08706v2)** | 2025-08-22 | <details><summary>15 pa...</summary><p>15 pages, 7 figures, 8 tables. ObjTac dataset: https://readerek.github.io/Objtac.github.io</p></details> |
| **[Survey of Vision-Language-Action Models for Embodied Manipulation](http://arxiv.org/abs/2508.15201v1)** | 2025-08-21 | in Chinese language |
| **[MinD: Learning A Dual-System World Model for Real-Time Planning and Implicit Risk Analysis](http://arxiv.org/abs/2506.18897v2)** | 2025-08-20 |  |
| **[3D-Generalist: Self-Improving Vision-Language-Action Models for Crafting 3D Worlds](http://arxiv.org/abs/2507.06484v2)** | 2025-08-19 | <details><summary>proje...</summary><p>project website: https://ai.stanford.edu/~sunfanyun/3d-generalist/</p></details> |
| **[CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models](http://arxiv.org/abs/2508.13446v1)** | 2025-08-19 |  |
| **[Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy](http://arxiv.org/abs/2508.13103v1)** | 2025-08-18 |  |

## robot
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation](http://arxiv.org/abs/2508.19236v1)** | 2025-08-26 | <details><summary>The p...</summary><p>The project is available at https://shihao1895.github.io/MemoryVLA</p></details> |
| **[Dojo: A Differentiable Physics Engine for Robotics](http://arxiv.org/abs/2203.00806v5)** | 2025-08-26 |  |
| **[AutoRing: Imitation Learning--based Autonomous Intraocular Foreign Body Removal Manipulation with Eye Surgical Robot](http://arxiv.org/abs/2508.19191v1)** | 2025-08-26 |  |
| **[Real-Time Model Checking for Closed-Loop Robot Reactive Planning](http://arxiv.org/abs/2508.19186v1)** | 2025-08-26 | <details><summary>30 pa...</summary><p>30 pages excluding references, 18 figures, submitted to Formal Aspects of Computing</p></details> |
| **[From Tabula Rasa to Emergent Abilities: Discovering Robot Skills via Real-World Unsupervised Quality-Diversity](http://arxiv.org/abs/2508.19172v1)** | 2025-08-26 | <details><summary>Accep...</summary><p>Accepted at CoRL 2025</p></details> |
| **[Uncertainty-Resilient Active Intention Recognition for Robotic Assistants](http://arxiv.org/abs/2508.19150v1)** | 2025-08-26 | <details><summary>(To a...</summary><p>(To appear) In Proceedings of ECMR 2025</p></details> |
| **[DELIVER: A System for LLM-Guided Coordinated Multi-Robot Pickup and Delivery using Voronoi-Based Relay Planning](http://arxiv.org/abs/2508.19114v1)** | 2025-08-26 | <details><summary>Submi...</summary><p>Submission under review at the 2026 IEEE/SICE International Symposium on System Integration (SII 2026)</p></details> |
| **[An LLM-powered Natural-to-Robotic Language Translation Framework with Correctness Guarantees](http://arxiv.org/abs/2508.19074v1)** | 2025-08-26 |  |
| **[HuBE: Cross-Embodiment Human-like Behavior Execution for Humanoid Robots](http://arxiv.org/abs/2508.19002v1)** | 2025-08-26 | <details><summary>8 pag...</summary><p>8 pages, 8 figures,4 tables</p></details> |
| **[VisionSafeEnhanced VPC: Cautious Predictive Control with Visibility Constraints under Uncertainty for Autonomous Robotic Surgery](http://arxiv.org/abs/2508.18937v1)** | 2025-08-26 | 8 pages, 6 figures |
| **[Enhancing Reusability of Learned Skills for Robot Manipulation via Gaze Information and Motion Bottlenecks](http://arxiv.org/abs/2502.18121v3)** | 2025-08-26 |  |
| **[Multi-Touch and Bending Perception Using Electrical Impedance Tomography for Robotics](http://arxiv.org/abs/2503.13048v2)** | 2025-08-26 |  |
| **[Enhancing Video-Based Robot Failure Detection Using Task Knowledge](http://arxiv.org/abs/2508.18705v1)** | 2025-08-26 | <details><summary>Accep...</summary><p>Accepted at ECMR 2025</p></details> |
| **[AgriChrono: A Multi-modal Dataset Capturing Crop Growth and Lighting Variability with a Field Robot](http://arxiv.org/abs/2508.18694v1)** | 2025-08-26 |  |
| **[Enhancing Multi-Robot Semantic Navigation Through Multimodal Chain-of-Thought Score Collaboration](http://arxiv.org/abs/2412.18292v5)** | 2025-08-26 | <details><summary>16 pa...</summary><p>16 pages, 10 figures, Extended Version of accepted AAAI 2025 Paper</p></details> |

## Vision Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Route-and-Execute: Auditable Model-Card Matching and Specialty-Level Deployment](http://arxiv.org/abs/2508.16839v2)** | 2025-08-26 |  |
| **[mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation](http://arxiv.org/abs/2505.24073v2)** | 2025-08-26 | 16 pages |
| **[Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary Perception in LVLMs](http://arxiv.org/abs/2508.19111v1)** | 2025-08-26 | EMNLP2025 Findings |
| **[Architecting Clinical Collaboration: Multi-Agent Reasoning Systems for Multimodal Medical VQA](http://arxiv.org/abs/2507.05520v3)** | 2025-08-26 |  |
| **[ProPy: Building Interactive Prompt Pyramids upon CLIP for Partially Relevant Video Retrieval](http://arxiv.org/abs/2508.19024v1)** | 2025-08-26 | <details><summary>Accep...</summary><p>Accepted by EMNLP 2025 Findings</p></details> |
| **[Ask Me Again Differently: GRAS for Measuring Bias in Vision Language Models on Gender, Race, Age, and Skin Tone](http://arxiv.org/abs/2508.18989v1)** | 2025-08-26 |  |
| **[Enhancing Document VQA Models via Retrieval-Augmented Generation](http://arxiv.org/abs/2508.18984v1)** | 2025-08-26 | <details><summary>Accep...</summary><p>Accepted at Workshop on Machine Learning in Document Analysis and Recognition (ICDAR WML 2025), Wuhan, China</p></details> |
| **[Mind the (Language) Gap: Towards Probing Numerical and Cross-Lingual Limits of LVLMs](http://arxiv.org/abs/2508.17334v2)** | 2025-08-26 |  |
| **[Prototype-Guided Diffusion: Visual Conditioning without External Memory](http://arxiv.org/abs/2508.09922v4)** | 2025-08-26 |  |
| **[M$^2$IV: Towards Efficient and Fine-grained Multimodal In-Context Learning via Representation Engineering](http://arxiv.org/abs/2504.04633v3)** | 2025-08-26 | <details><summary>COLM ...</summary><p>COLM 2025, 30 pages, 10 figures, 16 tables</p></details> |
| **[Video CLIP Model for Multi-View Echocardiography Interpretation](http://arxiv.org/abs/2504.18800v2)** | 2025-08-26 |  |
| **[Toward Robust Medical Fairness: Debiased Dual-Modal Alignment via Text-Guided Attribute-Disentangled Prompt Learning for Vision-Language Models](http://arxiv.org/abs/2508.18886v1)** | 2025-08-26 |  |
| **[Weakly-Supervised 3D Visual Grounding based on Visual Language Alignment](http://arxiv.org/abs/2312.09625v5)** | 2025-08-26 |  |
| **[Hidden Tail: Adversarial Image Causing Stealthy Resource Consumption in Vision-Language Models](http://arxiv.org/abs/2508.18805v1)** | 2025-08-26 |  |
| **[Rethinking Human-Object Interaction Evaluation for both Vision-Language Models and HOI-Specific Methods](http://arxiv.org/abs/2508.18753v1)** | 2025-08-26 |  |

## world model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[FlowVLA: Thinking in Motion with a Visual Chain of Thought](http://arxiv.org/abs/2508.18269v2)** | 2025-08-26 |  |
| **[Dream to Chat: Model-based Reinforcement Learning on Dialogues with User Belief Modeling](http://arxiv.org/abs/2508.16876v2)** | 2025-08-26 | <details><summary>Accep...</summary><p>Accepted to EMNLP 2025 Findings</p></details> |
| **[Language Models For Generalised PDDL Planning: Synthesising Sound and Programmatic Policies](http://arxiv.org/abs/2508.18507v1)** | 2025-08-25 | <details><summary>RLC 2...</summary><p>RLC 2025 Workshop on Programmatic Reinforcement Learning</p></details> |
| **[ParticleFormer: A 3D Point Cloud World Model for Multi-Object, Multi-Material Robotic Manipulation](http://arxiv.org/abs/2506.23126v4)** | 2025-08-25 |  |
| **[Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI](http://arxiv.org/abs/2407.06886v8)** | 2025-08-25 | <details><summary>The c...</summary><p>The comprehensive review of Embodied AI. We also provide the resource repository for Embodied AI: https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List</p></details> |
| **[GWM: Towards Scalable Gaussian World Models for Robotic Manipulation](http://arxiv.org/abs/2508.17600v1)** | 2025-08-25 | <details><summary>Publi...</summary><p>Published at ICCV 2025. Project page: https://gaussian-world-model.github.io/</p></details> |
| **[HERO: Hierarchical Extrapolation and Refresh for Efficient World Models](http://arxiv.org/abs/2508.17588v1)** | 2025-08-25 | 12 pages in total |
| **[Explain Before You Answer: A Survey on Compositional Visual Reasoning](http://arxiv.org/abs/2508.17298v1)** | 2025-08-24 |  |
| **[Seeing Clearly, Forgetting Deeply: Revisiting Fine-Tuned Video Generators for Driving Simulation](http://arxiv.org/abs/2508.16512v1)** | 2025-08-22 |  |
| **[WebEvolver: Enhancing Web Agent Self-Improvement with Coevolving World Model](http://arxiv.org/abs/2504.21024v2)** | 2025-08-21 | <details><summary>EMNLP...</summary><p>EMNLP 2025 Main Conference</p></details> |
| **[Goals and the Structure of Experience](http://arxiv.org/abs/2508.15013v1)** | 2025-08-20 |  |
| **[VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness](http://arxiv.org/abs/2503.21755v2)** | 2025-08-20 | <details><summary>Equal...</summary><p>Equal contributions from first two authors. Project page: https://vchitect.github.io/VBench-2.0-project/ Code: https://github.com/Vchitect/VBench</p></details> |
| **[MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers](http://arxiv.org/abs/2508.14704v1)** | 2025-08-20 | <details><summary>Websi...</summary><p>Website: https://mcp-universe.github.io</p></details> |
| **[Beyond Individuals: Collective Predictive Coding for Memory, Attention, and the Emergence of Language](http://arxiv.org/abs/2508.15859v1)** | 2025-08-20 |  |
| **[Extremum Flow Matching for Offline Goal Conditioned Reinforcement Learning](http://arxiv.org/abs/2505.19717v2)** | 2025-08-20 | <details><summary>2025 ...</summary><p>2025 IEEE-RAS 24th International Conference on Humanoid Robots (Humanoids), Sep 2025, Seoul, South Korea</p></details> |

