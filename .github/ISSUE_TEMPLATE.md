---
title: Latest 15 Papers - September 02, 2025
labels: documentation
---
**Please check the [Github](https://github.com/Ed1sonChen/DailyArxiv) page for a better reading experience and more papers.**

## Vision Language Action
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification](http://arxiv.org/abs/2508.21046v1)** | 2025-08-28 | <details><summary>23 pa...</summary><p>23 pages, 8 figures, Project Page: https://jiutian-vl.github.io/CogVLA-page</p></details> |
| **[EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control](http://arxiv.org/abs/2508.21112v1)** | 2025-08-28 |  |
| **[Pixel Motion as Universal Representation for Robot Control](http://arxiv.org/abs/2505.07817v2)** | 2025-08-28 |  |
| **[Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation](http://arxiv.org/abs/2508.19958v2)** | 2025-08-28 | <details><summary>Accep...</summary><p>Accepted to CoRL 2025; Github Page: https://long-vla.github.io</p></details> |
| **[Ego-centric Predictive Model Conditioned on Hand Trajectories](http://arxiv.org/abs/2508.19852v2)** | 2025-08-28 | <details><summary>Code:...</summary><p>Code: github.com/showlab/Ego-PM</p></details> |
| **[Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies](http://arxiv.org/abs/2508.20072v1)** | 2025-08-27 | 15 pages |
| **[GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data](http://arxiv.org/abs/2505.03233v3)** | 2025-08-27 |  |
| **[MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation](http://arxiv.org/abs/2508.19236v1)** | 2025-08-26 | <details><summary>The p...</summary><p>The project is available at https://shihao1895.github.io/MemoryVLA</p></details> |
| **[FlowVLA: Thinking in Motion with a Visual Chain of Thought](http://arxiv.org/abs/2508.18269v2)** | 2025-08-26 |  |
| **[DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge](http://arxiv.org/abs/2507.04447v3)** | 2025-08-26 |  |
| **[MapleGrasp: Mask-guided Feature Pooling for Language-driven Efficient Robotic Grasping](http://arxiv.org/abs/2506.06535v3)** | 2025-08-25 |  |
| **[Continual Learning for Generative AI: From LLMs to MLLMs and Beyond](http://arxiv.org/abs/2506.13045v4)** | 2025-08-24 | Preprint |
| **[4D Visual Pre-training for Robot Learning](http://arxiv.org/abs/2508.17230v1)** | 2025-08-24 |  |
| **[GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions](http://arxiv.org/abs/2508.07650v2)** | 2025-08-23 | 10 pages, 6 figures |
| **[NinA: Normalizing Flows in Action. Training VLA Models with Normalizing Flows](http://arxiv.org/abs/2508.16845v1)** | 2025-08-23 |  |

## robot
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[COBRA-PPM: A Causal Bayesian Reasoning Architecture Using Probabilistic Programming for Robot Manipulation Under Uncertainty](http://arxiv.org/abs/2403.14488v4)** | 2025-08-29 | <details><summary>8 pag...</summary><p>8 pages, 7 figures, accepted to the 2025 IEEE European Conference on Mobile Robots (ECMR 2025)</p></details> |
| **[Centralization vs. decentralization in multi-robot sweep coverage with ground robots and UAVs](http://arxiv.org/abs/2408.06553v4)** | 2025-08-29 | <details><summary>IRIDI...</summary><p>IRIDIA, Universite Libre de Bruxelles, Brussels, Belgium, 2021</p></details> |
| **[Can a mobile robot learn from a pedestrian model to prevent the sidewalk salsa?](http://arxiv.org/abs/2508.21690v1)** | 2025-08-29 |  |
| **[Robust Convex Model Predictive Control with collision avoidance guarantees for robot manipulators](http://arxiv.org/abs/2508.21677v1)** | 2025-08-29 |  |
| **[The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics](http://arxiv.org/abs/2508.21635v1)** | 2025-08-29 | <details><summary>First...</summary><p>First published on The International Journal of Robotics Research: https://journals.sagepub.com/doi/10.1177/02783649251368909</p></details> |
| **[Knowledge in multi-robot systems: an interplay of dynamics, computation and communication](http://arxiv.org/abs/2501.18309v2)** | 2025-08-29 |  |
| **[CoRI: Communication of Robot Intent for Physical Human-Robot Interaction](http://arxiv.org/abs/2505.20537v2)** | 2025-08-29 | <details><summary>To be...</summary><p>To be published in Proceedings of the 9th Conference on Robot Learning (CoRL). 34 pages, 10 figures</p></details> |
| **[Assessing Human Cooperation for Enhancing Social Robot Navigation](http://arxiv.org/abs/2508.21455v1)** | 2025-08-29 |  |
| **[RoboInspector: Unveiling the Unreliability of Policy Code for LLM-enabled Robotic Manipulation](http://arxiv.org/abs/2508.21378v1)** | 2025-08-29 |  |
| **[QuaDreamer: Controllable Panoramic Video Generation for Quadruped Robots](http://arxiv.org/abs/2508.02512v2)** | 2025-08-29 | <details><summary>Accep...</summary><p>Accepted to CoRL 2025. The source code and model weights will be publicly available at \url{https://github.com/losehu/QuaDreamer</p></details> |
| **[UltraTac: Integrated Ultrasound-Augmented Visuotactile Sensor for Enhanced Robotic Perception](http://arxiv.org/abs/2508.20982v2)** | 2025-08-29 | <details><summary>Accep...</summary><p>Accepted to IROS 2025</p></details> |
| **[Towards Embodiment Scaling Laws in Robot Locomotion](http://arxiv.org/abs/2505.05753v2)** | 2025-08-29 | <details><summary>Confe...</summary><p>Conference on Robot Learning (CoRL), 2025. Project website: https://embodiment-scaling-laws.github.io/</p></details> |
| **[Multi-robot Path Planning and Scheduling via Model Predictive Optimal Transport (MPC-OT)](http://arxiv.org/abs/2508.21205v1)** | 2025-08-28 | <details><summary>2025 ...</summary><p>2025 IEEE Conference on Decision and Control</p></details> |
| **[Pellet-based 3D Printing of Soft Thermoplastic Elastomeric Membranes for Soft Robotic Applications](http://arxiv.org/abs/2503.20957v2)** | 2025-08-28 |  |
| **[HITTER: A HumanoId Table TEnnis Robot via Hierarchical Planning and Learning](http://arxiv.org/abs/2508.21043v1)** | 2025-08-28 | 8 pages, 7 figures |

## Vision Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[VoCap: Video Object Captioning and Segmentation from Any Prompt](http://arxiv.org/abs/2508.21809v1)** | 2025-08-29 |  |
| **[CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models](http://arxiv.org/abs/2508.21732v1)** | 2025-08-29 |  |
| **[CoRI: Communication of Robot Intent for Physical Human-Robot Interaction](http://arxiv.org/abs/2505.20537v2)** | 2025-08-29 | <details><summary>To be...</summary><p>To be published in Proceedings of the 9th Conference on Robot Learning (CoRL). 34 pages, 10 figures</p></details> |
| **[How Well Do Vision--Language Models Understand Cities? A Comparative Study on Spatial Reasoning from Street-View Images](http://arxiv.org/abs/2508.21565v1)** | 2025-08-29 | <details><summary>Accep...</summary><p>Accepted to ICCV Workshop 2025</p></details> |
| **[HCCM: Hierarchical Cross-Granularity Contrastive and Matching Learning for Natural Language-Guided Drones](http://arxiv.org/abs/2508.21539v1)** | 2025-08-29 | <details><summary>Accep...</summary><p>Accepted by ACM MM'25</p></details> |
| **[PlantVillageVQA: A Visual Question Answering Dataset for Benchmarking Vision-Language Models in Plant Science](http://arxiv.org/abs/2508.17117v2)** | 2025-08-28 | <details><summary>17 pa...</summary><p>17 pages, 15 figures and Submittd to Nature Scientific Data</p></details> |
| **[OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning](http://arxiv.org/abs/2508.21066v1)** | 2025-08-28 | <details><summary>proje...</summary><p>project url: https://one-reward.github.io</p></details> |
| **[CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification](http://arxiv.org/abs/2508.21046v1)** | 2025-08-28 | <details><summary>23 pa...</summary><p>23 pages, 8 figures, Project Page: https://jiutian-vl.github.io/CogVLA-page</p></details> |
| **[Exploring Typographic Visual Prompts Injection Threats in Cross-Modality Generation Models](http://arxiv.org/abs/2503.11519v3)** | 2025-08-28 | <details><summary>This ...</summary><p>This paper is accepted by IJCAI2025 Workshop on Deepfake Detection, Localization, and Interpretability</p></details> |
| **[Learning Primitive Embodied World Models: Towards Scalable Robotic Learning](http://arxiv.org/abs/2508.20840v1)** | 2025-08-28 |  |
| **[Estimating 2D Keypoints of Surgical Tools Using Vision-Language Models with Low-Rank Adaptation](http://arxiv.org/abs/2508.20830v1)** | 2025-08-28 | <details><summary>Accep...</summary><p>Accepted to MICCAI 2025</p></details> |
| **[Evaluating Compositional Generalisation in VLMs and Diffusion Models](http://arxiv.org/abs/2508.20783v1)** | 2025-08-28 | <details><summary>11 pa...</summary><p>11 pages including references, 6 figures. Accepted at IWCS 2025</p></details> |
| **[Occlusion Robustness of CLIP for Military Vehicle Classification](http://arxiv.org/abs/2508.20760v1)** | 2025-08-28 | <details><summary>To be...</summary><p>To be presented at SPIE: Sensors + Imaging, Artificial Intelligence for Security and Defence Applications II</p></details> |
| **[NLKI: A lightweight Natural Language Knowledge Integration Framework for Improving Small VLMs in Commonsense VQA Tasks](http://arxiv.org/abs/2508.19724v2)** | 2025-08-28 |  |
| **["Humor, Art, or Misinformation?": A Multimodal Dataset for Intent-Aware Synthetic Image Detection](http://arxiv.org/abs/2508.20670v1)** | 2025-08-28 |  |

## world model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Learning Primitive Embodied World Models: Towards Scalable Robotic Learning](http://arxiv.org/abs/2508.20840v1)** | 2025-08-28 |  |
| **[Disentangled World Models: Learning to Transfer Semantic Knowledge from Distracting Videos for Reinforcement Learning](http://arxiv.org/abs/2503.08751v2)** | 2025-08-28 |  |
| **[MIDAS: Multimodal Interactive Digital-humAn Synthesis via Real-time Autoregressive Video Generation](http://arxiv.org/abs/2508.19320v2)** | 2025-08-28 | <details><summary>Techn...</summary><p>Technical Report. Project Page: https://chenmingthu.github.io/milm/</p></details> |
| **[Dynamics-Aligned Latent Imagination in Contextual World Models for Zero-Shot Generalization](http://arxiv.org/abs/2508.20294v1)** | 2025-08-27 | 31 pages, 4 figures |
| **[Possible Principles for Aligned Structure Learning Agents](http://arxiv.org/abs/2410.00258v3)** | 2025-08-27 | <details><summary>24 pa...</summary><p>24 pages of content, 33 with references; accepted version</p></details> |
| **[General agents contain world models](http://arxiv.org/abs/2506.01622v3)** | 2025-08-27 | <details><summary>Accep...</summary><p>Accepted ICML 2025. Typos corrected</p></details> |
| **[Tracking World States with Language Models: State-Based Evaluation Using Chess](http://arxiv.org/abs/2508.19851v1)** | 2025-08-27 | <details><summary>Spotl...</summary><p>Spotlight presentation at ICML 2025 Workshop on Assessing World Models</p></details> |
| **[Explain Before You Answer: A Survey on Compositional Visual Reasoning](http://arxiv.org/abs/2508.17298v2)** | 2025-08-27 | <details><summary>Proje...</summary><p>Project Page: https://github.com/pokerme7777/Compositional-Visual-Reasoning-Survey</p></details> |
| **[Enhancing Sample Efficiency and Exploration in Reinforcement Learning through the Integration of Diffusion Models and Proximal Policy Optimization](http://arxiv.org/abs/2409.01427v5)** | 2025-08-26 |  |
| **[FlowVLA: Thinking in Motion with a Visual Chain of Thought](http://arxiv.org/abs/2508.18269v2)** | 2025-08-26 |  |
| **[Dream to Chat: Model-based Reinforcement Learning on Dialogues with User Belief Modeling](http://arxiv.org/abs/2508.16876v2)** | 2025-08-26 | <details><summary>Accep...</summary><p>Accepted to EMNLP 2025 Findings</p></details> |
| **[Language Models For Generalised PDDL Planning: Synthesising Sound and Programmatic Policies](http://arxiv.org/abs/2508.18507v1)** | 2025-08-25 | <details><summary>RLC 2...</summary><p>RLC 2025 Workshop on Programmatic Reinforcement Learning</p></details> |
| **[ParticleFormer: A 3D Point Cloud World Model for Multi-Object, Multi-Material Robotic Manipulation](http://arxiv.org/abs/2506.23126v4)** | 2025-08-25 |  |
| **[Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI](http://arxiv.org/abs/2407.06886v8)** | 2025-08-25 | <details><summary>The c...</summary><p>The comprehensive review of Embodied AI. We also provide the resource repository for Embodied AI: https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List</p></details> |
| **[GWM: Towards Scalable Gaussian World Models for Robotic Manipulation](http://arxiv.org/abs/2508.17600v1)** | 2025-08-25 | <details><summary>Publi...</summary><p>Published at ICCV 2025. Project page: https://gaussian-world-model.github.io/</p></details> |

