---
title: Latest 15 Papers - March 19, 2025
labels: documentation
---
**Please check the [Github](https://github.com/Ed1sonChen/DailyArxiv) page for a better reading experience and more papers.**

## MLLM
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM](http://arxiv.org/abs/2503.14478v1)** | 2025-03-18 | <details><summary>Evalu...</summary><p>Evaluation Code and dataset see https://github.com/open-compass/Creation-MMBench</p></details> |
| **[Towards Self-Improving Systematic Cognition for Next-Generation Foundation MLLMs](http://arxiv.org/abs/2503.12303v2)** | 2025-03-18 | <details><summary>38 pa...</summary><p>38 pages. Preprint, work in progress</p></details> |
| **[MP-GUI: Modality Perception with MLLMs for GUI Understanding](http://arxiv.org/abs/2503.14021v1)** | 2025-03-18 | <details><summary>Paper...</summary><p>Paper accepted to CVPR 2025</p></details> |
| **[Conformal Prediction and MLLM aided Uncertainty Quantification in Scene Graph Generation](http://arxiv.org/abs/2503.13947v1)** | 2025-03-18 | <details><summary>Accep...</summary><p>Accepted at CVPR 2025</p></details> |
| **[DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs for Knowledge-Intensive Visual Grounding](http://arxiv.org/abs/2503.12797v2)** | 2025-03-18 |  |
| **[Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference](http://arxiv.org/abs/2503.13108v1)** | 2025-03-17 |  |
| **[Mitigating Cross-Modal Distraction and Ensuring Geometric Feasibility via Affordance-Guided, Self-Consistent MLLMs for Food Preparation Task Planning](http://arxiv.org/abs/2503.13055v1)** | 2025-03-17 |  |
| **[Efficient Motion-Aware Video MLLM](http://arxiv.org/abs/2503.13016v1)** | 2025-03-17 | CVPR 2025 |
| **[Empowering Source-Free Domain Adaptation via MLLM-Guided Reliability-Based Curriculum Learning](http://arxiv.org/abs/2405.18376v2)** | 2025-03-17 |  |
| **[Filter, Correlate, Compress: Training-Free Token Reduction for MLLM Acceleration](http://arxiv.org/abs/2411.17686v3)** | 2025-03-14 |  |
| **[CINEMA: Coherent Multi-Subject Video Generation via MLLM-Based Guidance](http://arxiv.org/abs/2503.10391v1)** | 2025-03-13 |  |
| **[LVAgent: Long Video Understanding by Multi-Round Dynamical Collaboration of MLLM Agents](http://arxiv.org/abs/2503.10200v1)** | 2025-03-13 |  |
| **[CAD-MLLM: Unifying Multimodality-Conditioned CAD Generation With MLLM](http://arxiv.org/abs/2411.04954v2)** | 2025-03-13 | <details><summary>Proje...</summary><p>Project page: https://cad-mllm.github.io/</p></details> |
| **[Information Density Principle for MLLM Benchmarks](http://arxiv.org/abs/2503.10079v1)** | 2025-03-13 |  |
| **[UVE: Are MLLMs Unified Evaluators for AI-Generated Videos?](http://arxiv.org/abs/2503.09949v1)** | 2025-03-13 |  |

## Vision Language Action
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation](http://arxiv.org/abs/2503.13446v1)** | 2025-03-17 | <details><summary>Accep...</summary><p>Accepted to CVPR 2025. Project Page: https://gary3410.github.io/momanipVLA/</p></details> |
| **[Dita: Scaling Diffusion Transformer for Generalist Vision-Language-Action Policy](http://arxiv.org/abs/2410.15959v5)** | 2025-03-17 | <details><summary>I wan...</summary><p>I want to withdraw the recent replacement (v4), given that the author is different, the title is also different and the content is totally different</p></details> |
| **[HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model](http://arxiv.org/abs/2503.10631v2)** | 2025-03-17 |  |
| **[ReVLA: Reverting Visual Domain Limitation of Robotic Foundation Models](http://arxiv.org/abs/2409.15250v2)** | 2025-03-13 | <details><summary>Accep...</summary><p>Accepted at ICRA-2025, Atlanta</p></details> |
| **[CombatVLA: An Efficient Vision-Language-Action Model for Combat Tasks in 3D Action Role-Playing Games](http://arxiv.org/abs/2503.09527v1)** | 2025-03-12 |  |
| **[QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped Robot Learning](http://arxiv.org/abs/2412.15576v3)** | 2025-03-11 | <details><summary>Accep...</summary><p>Accepted to ICRA 2025; Github page: https://quart-online.github.io</p></details> |
| **[OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature Extraction](http://arxiv.org/abs/2503.03734v2)** | 2025-03-11 |  |
| **[MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models](http://arxiv.org/abs/2503.08007v1)** | 2025-03-11 | <details><summary>Accep...</summary><p>Accepted by ICRA 2025</p></details> |
| **[PointVLA: Injecting the 3D World into Vision-Language-Action Models](http://arxiv.org/abs/2503.07511v1)** | 2025-03-10 |  |
| **[Exploring the Adversarial Vulnerabilities of Vision-Language-Action Models in Robotics](http://arxiv.org/abs/2411.13587v3)** | 2025-03-10 | <details><summary>Githu...</summary><p>Github: https://github.com/William-wAng618/roboticAttack Homepage: https://vlaattacker.github.io/</p></details> |
| **[Robotic Control via Embodied Chain-of-Thought Reasoning](http://arxiv.org/abs/2407.08693v3)** | 2025-03-06 | <details><summary>Proje...</summary><p>Project Website: https://embodied-cot.github.io. Updated funding information</p></details> |
| **[Refined Policy Distillation: From VLA Generalists to RL Experts](http://arxiv.org/abs/2503.05833v1)** | 2025-03-06 |  |
| **[VLA Model-Expert Collaboration for Bi-directional Manipulation Learning](http://arxiv.org/abs/2503.04163v1)** | 2025-03-06 |  |
| **[DexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping](http://arxiv.org/abs/2502.20900v2)** | 2025-03-05 | 21 pages, 10 figures |
| **[CLIP-RT: Learning Language-Conditioned Robotic Policies from Natural Language Supervision](http://arxiv.org/abs/2411.00508v3)** | 2025-03-05 | 27 pages |

