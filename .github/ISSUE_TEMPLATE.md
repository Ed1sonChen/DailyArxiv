---
title: Latest 15 Papers - September 25, 2025
labels: documentation
---
**Please check the [Github](https://github.com/Ed1sonChen/DailyArxiv) page for a better reading experience and more papers.**

## Vision Language Action
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving](http://arxiv.org/abs/2509.20109v1)** | 2025-09-24 |  |
| **[FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models](http://arxiv.org/abs/2509.19870v1)** | 2025-09-24 |  |
| **[Beyond Human Demonstrations: Diffusion-Based Reinforcement Learning to Generate Data for VLA Training](http://arxiv.org/abs/2509.19752v1)** | 2025-09-24 |  |
| **[Agentic Scene Policies: Unifying Space, Semantics, and Affordances for Robot Action](http://arxiv.org/abs/2509.19571v1)** | 2025-09-23 | <details><summary>Proje...</summary><p>Project page: https://montrealrobotics.ca/agentic-scene-policies.github.io/</p></details> |
| **[OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation](http://arxiv.org/abs/2509.19480v1)** | 2025-09-23 | <details><summary>9 pag...</summary><p>9 pages, 7 figures, 6 tables</p></details> |
| **[Pure Vision Language Action (VLA) Models: A Comprehensive Survey](http://arxiv.org/abs/2509.19012v1)** | 2025-09-23 |  |
| **[Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations](http://arxiv.org/abs/2509.18953v1)** | 2025-09-23 |  |
| **[Latent Action Pretraining Through World Modeling](http://arxiv.org/abs/2509.18428v1)** | 2025-09-22 |  |
| **[GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model](http://arxiv.org/abs/2509.14117v2)** | 2025-09-22 | Under Review |
| **[VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model](http://arxiv.org/abs/2509.09372v2)** | 2025-09-22 | <details><summary>28 pa...</summary><p>28 pages; Project page: https://vla-adapter.github.io/; Github: https://github.com/OpenHelix-Team/VLA-Adapter; HuggingFace: https://huggingface.co/VLA-Adapter</p></details> |
| **[The Better You Learn, The Smarter You Prune: Towards Efficient Vision-language-action Models via Differentiable Token Pruning](http://arxiv.org/abs/2509.12594v2)** | 2025-09-21 | <details><summary>Under...</summary><p>Under review. Project site: https://liauto-research.github.io/LightVLA</p></details> |
| **[Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse](http://arxiv.org/abs/2506.07639v2)** | 2025-09-21 |  |
| **[Spec-VLA: Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance](http://arxiv.org/abs/2507.22424v2)** | 2025-09-20 | <details><summary>13 pa...</summary><p>13 pages, 5 figures, Accepted by EMNLP 2025 (main conference)</p></details> |
| **[Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding](http://arxiv.org/abs/2507.00416v2)** | 2025-09-20 |  |
| **[CoReVLA: A Dual-Stage End-to-End Autonomous Driving Framework for Long-Tail Scenarios via Collect-and-Refine](http://arxiv.org/abs/2509.15968v1)** | 2025-09-19 |  |

## robot
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Data-fused Model Predictive Control with Guarantees: Application to Flying Humanoid Robots](http://arxiv.org/abs/2509.10353v3)** | 2025-09-24 | 8 pages, 3 figures |
| **[A Biomimetic Vertebraic Soft Robotic Tail for High-Speed, High-Force Dynamic Maneuvering](http://arxiv.org/abs/2509.20219v1)** | 2025-09-24 | <details><summary>20 pa...</summary><p>20 pages, 11 figures, 4 tables. Submitted Under Review</p></details> |
| **[U-ARM : Ultra low-cost general teleoperation interface for robot manipulation](http://arxiv.org/abs/2509.02437v2)** | 2025-09-24 |  |
| **[Orbital Stabilization and Time Synchronization of Unstable Periodic Motions in Underactuated Robots](http://arxiv.org/abs/2509.20082v1)** | 2025-09-24 |  |
| **[Queryable 3D Scene Representation: A Multi-Modal Framework for Semantic Reasoning and Robotic Task Planning](http://arxiv.org/abs/2509.20077v1)** | 2025-09-24 |  |
| **[LLM Trainer: Automated Robotic Data Generating via Demonstration Augmentation using LLMs](http://arxiv.org/abs/2509.20070v1)** | 2025-09-24 | <details><summary>9 pag...</summary><p>9 pages, 5 figures, 4 tables. Submitted to ICRA 2026</p></details> |
| **[MARG: MAstering Risky Gap Terrains for Legged Robots with Elevation Mapping](http://arxiv.org/abs/2509.20036v1)** | 2025-09-24 |  |
| **[TrojanRobot: Physical-world Backdoor Attacks Against VLM-based Robotic Manipulation](http://arxiv.org/abs/2411.11683v6)** | 2025-09-24 |  |
| **[Generalist Robot Manipulation beyond Action Labeled Data](http://arxiv.org/abs/2509.19958v1)** | 2025-09-24 | <details><summary>Accep...</summary><p>Accepted at Conference on Robot Learning 2025</p></details> |
| **[Robot Trajectron V2: A Probabilistic Shared Control Framework for Navigation](http://arxiv.org/abs/2509.19954v1)** | 2025-09-24 | 26 pages, 20 figures |
| **[SEM: Enhancing Spatial Understanding for Robust Robot Manipulation](http://arxiv.org/abs/2505.16196v3)** | 2025-09-24 |  |
| **[VLM See, Robot Do: Human Demo Video to Robot Action Plan via Vision Language Model](http://arxiv.org/abs/2410.08792v2)** | 2025-09-24 |  |
| **[ByteWrist: A Parallel Robotic Wrist Enabling Flexible and Anthropomorphic Motion for Confined Spaces](http://arxiv.org/abs/2509.18084v2)** | 2025-09-24 | <details><summary>Tech ...</summary><p>Tech Report.13 pages, 9 figures. Project page: https://bytewrist.github.io/</p></details> |
| **[Active Shadowing (ASD): Manipulating Perception of Robotic Behaviors via Implicit Virtual Communication](http://arxiv.org/abs/2407.01468v3)** | 2025-09-23 |  |
| **[Look as You Leap: Planning Simultaneous Motion and Perception for High-DOF Robots](http://arxiv.org/abs/2509.19610v1)** | 2025-09-23 | <details><summary>16 pa...</summary><p>16 pages, 10 figures, under review</p></details> |

## Vision Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Multimodal Reference Visual Grounding](http://arxiv.org/abs/2504.02876v2)** | 2025-09-24 | <details><summary>Proje...</summary><p>Project page with our code and dataset: https://irvlutd.github.io/MultiGrounding</p></details> |
| **[Enhancing Targeted Adversarial Attacks on Large Vision-Language Models via Intermediate Projector](http://arxiv.org/abs/2508.13739v2)** | 2025-09-24 |  |
| **[A co-evolving agentic AI system for medical imaging analysis](http://arxiv.org/abs/2509.20279v1)** | 2025-09-24 |  |
| **[Deciphering Functions of Neurons in Vision-Language Models](http://arxiv.org/abs/2502.18485v4)** | 2025-09-24 | <details><summary>Accep...</summary><p>Accepted by the 31st ACM International Conference on Multimedia (ACM MM 2025)</p></details> |
| **[Universal Camouflage Attack on Vision-Language Models for Autonomous Driving](http://arxiv.org/abs/2509.20196v1)** | 2025-09-24 |  |
| **[VisualTrap: A Stealthy Backdoor Attack on GUI Agents via Visual Grounding Manipulation](http://arxiv.org/abs/2507.06899v2)** | 2025-09-24 | Accepted in COLM2025 |
| **[EchoBench: Benchmarking Sycophancy in Medical Large Vision-Language Models](http://arxiv.org/abs/2509.20146v1)** | 2025-09-24 | 29 pages, 6 figures |
| **[GraphEQA: Using 3D Semantic Scene Graphs for Real-time Embodied Question Answering](http://arxiv.org/abs/2412.14480v2)** | 2025-09-24 | <details><summary>Proje...</summary><p>Project website: https://saumyasaxena.github.io/grapheqa</p></details> |
| **[A Simple Data Augmentation Strategy for Text-in-Image Scientific VQA](http://arxiv.org/abs/2509.20119v1)** | 2025-09-24 | <details><summary>Accep...</summary><p>Accepted at WiNLP, 2025</p></details> |
| **[Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving](http://arxiv.org/abs/2509.20109v1)** | 2025-09-24 |  |
| **[Queryable 3D Scene Representation: A Multi-Modal Framework for Semantic Reasoning and Robotic Task Planning](http://arxiv.org/abs/2509.20077v1)** | 2025-09-24 |  |
| **[To Trust Or Not To Trust Your Vision-Language Model's Prediction](http://arxiv.org/abs/2505.23745v2)** | 2025-09-24 |  |
| **[OmniScene: Attention-Augmented Multimodal 4D Scene Understanding for Autonomous Driving](http://arxiv.org/abs/2509.19973v1)** | 2025-09-24 |  |
| **[Generalist Robot Manipulation beyond Action Labeled Data](http://arxiv.org/abs/2509.19958v1)** | 2025-09-24 | <details><summary>Accep...</summary><p>Accepted at Conference on Robot Learning 2025</p></details> |
| **[Benchmarking Gaslighting Attacks Against Speech Large Language Models](http://arxiv.org/abs/2509.19858v1)** | 2025-09-24 | <details><summary>5 pag...</summary><p>5 pages, 2 figures, 3 tables</p></details> |

## world model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling](http://arxiv.org/abs/2509.12201v2)** | 2025-09-24 | <details><summary>https...</summary><p>https://yangzhou24.github.io/OmniWorld/</p></details> |
| **[Embodied AI: From LLMs to World Models](http://arxiv.org/abs/2509.20021v1)** | 2025-09-24 | <details><summary>Accep...</summary><p>Accepted by IEEE CASM</p></details> |
| **[AnySafe: Adapting Latent Safety Filters at Runtime via Safety Constraint Parameterization in the Latent Space](http://arxiv.org/abs/2509.19555v1)** | 2025-09-23 |  |
| **[DAWM: Diffusion Action World Models for Offline Reinforcement Learning via Action-Inferred Transitions](http://arxiv.org/abs/2509.19538v1)** | 2025-09-23 | <details><summary>ICML2...</summary><p>ICML2025 workshop Building Physically Plausible World Models</p></details> |
| **[Uncertainty-aware Latent Safety Filters for Avoiding Out-of-Distribution Failures](http://arxiv.org/abs/2505.00779v2)** | 2025-09-23 | <details><summary>Confe...</summary><p>Conference on Robot Learning (CoRL 2025)</p></details> |
| **[PIGDreamer: Privileged Information Guided World Models for Safe Partially Observable Reinforcement Learning](http://arxiv.org/abs/2508.02159v2)** | 2025-09-23 | ICML 2025 |
| **[Probing LLM World Models: Enhancing Guesstimation with Wisdom of Crowds Decoding](http://arxiv.org/abs/2501.17310v4)** | 2025-09-23 |  |
| **[World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation](http://arxiv.org/abs/2509.19080v1)** | 2025-09-23 |  |
| **[Program Synthesis via Test-Time Transduction](http://arxiv.org/abs/2509.17393v2)** | 2025-09-23 | NeurIPS 2025 |
| **[Latent Action Pretraining Through World Modeling](http://arxiv.org/abs/2509.18428v1)** | 2025-09-22 |  |
| **[Remote Sensing-Oriented World Model](http://arxiv.org/abs/2509.17808v1)** | 2025-09-22 | 10 pages, 5 figures |
| **[Latent Policy Steering with Embodiment-Agnostic Pretrained World Models](http://arxiv.org/abs/2507.13340v2)** | 2025-09-21 |  |
| **[Learning Primitive Embodied World Models: Towards Scalable Robotic Learning](http://arxiv.org/abs/2508.20840v2)** | 2025-09-20 |  |
| **[Foundation Models as World Models: A Foundational Study in Text-Based GridWorlds](http://arxiv.org/abs/2509.15915v1)** | 2025-09-19 | <details><summary>20 pa...</summary><p>20 pages, 9 figures. Accepted for presentation at the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop on Embodied World Models for Decision Making</p></details> |
| **[World Modelling Improves Language Model Agents](http://arxiv.org/abs/2506.02918v2)** | 2025-09-19 |  |

