---
title: Latest 15 Papers - December 08, 2025
labels: documentation
---
**Please check the [Github](https://github.com/Ed1sonChen/DailyArxiv) page for a better reading experience and more papers.**

## Vision Language Action
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Training-Time Action Conditioning for Efficient Real-Time Chunking](https://arxiv.org/abs/2512.05964v1)** | 2025-12-05 |  |
| **[SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models](https://arxiv.org/abs/2512.05955v1)** | 2025-12-05 |  |
| **[TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.05943v1)** | 2025-12-05 |  |
| **[Probing the effectiveness of World Models for Spatial Reasoning through Test-time Scaling](https://arxiv.org/abs/2512.05809v1)** | 2025-12-05 | <details><summary>Exten...</summary><p>Extended abstract at World Modeling Workshop 2026</p></details> |
| **[HiMoE-VLA: Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies](https://arxiv.org/abs/2512.05693v1)** | 2025-12-05 |  |
| **[ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic Grounding for Generalizable Robotic Manipulation](https://arxiv.org/abs/2507.18262v3)** | 2025-12-05 | 12 pages,9 figures |
| **[Conscious Gaze: Adaptive Attention Mechanisms for Hallucination Mitigation in Vision-Language Models](https://arxiv.org/abs/2512.05546v1)** | 2025-12-05 | 6 pages, 6 figures |
| **[VOST-SGG: VLM-Aided One-Stage Spatio-Temporal Scene Graph Generation](https://arxiv.org/abs/2512.05524v1)** | 2025-12-05 |  |
| **[Real-Time Execution of Action Chunking Flow Policies](https://arxiv.org/abs/2506.07339v2)** | 2025-12-05 | <details><summary>publi...</summary><p>published in NeurIPS 2025</p></details> |
| **[Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment](https://arxiv.org/abs/2511.04555v2)** | 2025-12-05 | <details><summary>Githu...</summary><p>Github: https://github.com/MINT-SJTU/Evo-1</p></details> |
| **[Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views](https://arxiv.org/abs/2511.12878v3)** | 2025-12-05 | <details><summary>Exten...</summary><p>Extended journal version of MMTwin (IROS'25). Code and data: https://github.com/IRMVLab/UniHand</p></details> |
| **[From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model](https://arxiv.org/abs/2512.05277v1)** | 2025-12-04 |  |
| **[SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control](https://arxiv.org/abs/2511.07820v2)** | 2025-12-04 | <details><summary>Proje...</summary><p>Project page: https://nvlabs.github.io/SONIC/</p></details> |
| **[STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models](https://arxiv.org/abs/2512.05107v1)** | 2025-12-04 |  |
| **[TV2TV: A Unified Framework for Interleaved Language and Video Generation](https://arxiv.org/abs/2512.05103v1)** | 2025-12-04 |  |

## robot
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Real-time Remote Tracking and Autonomous Planning for Whale Rendezvous using Robots](https://arxiv.org/abs/2512.05808v1)** | 2025-12-05 |  |
| **[3D Path Planning for Robot-assisted Vertebroplasty from Arbitrary Bi-plane X-ray via Differentiable Rendering](https://arxiv.org/abs/2512.05803v1)** | 2025-12-05 |  |
| **[Learning Visually Interpretable Oscillator Networks for Soft Continuum Robots from Video](https://arxiv.org/abs/2511.18322v2)** | 2025-12-05 | <details><summary>Datas...</summary><p>Dataset available at: https://zenodo.org/records/17812071</p></details> |
| **[An Integrated System for WEEE Sorting Employing X-ray Imaging, AI-based Object Detection and Segmentation, and Delta Robot Manipulation](https://arxiv.org/abs/2512.05599v1)** | 2025-12-05 |  |
| **[ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic Grounding for Generalizable Robotic Manipulation](https://arxiv.org/abs/2507.18262v3)** | 2025-12-05 | 12 pages,9 figures |
| **[A Hyperspectral Imaging Guided Robotic Grasping System](https://arxiv.org/abs/2512.05578v1)** | 2025-12-05 | <details><summary>8 pag...</summary><p>8 pages, 7 figures, Accepted to IEEE Robotics and Automation Letters (RA-L) 2025</p></details> |
| **[Spatiotemporal Tubes for Differential Drive Robots with Model Uncertainty](https://arxiv.org/abs/2512.05495v1)** | 2025-12-05 |  |
| **[H-GAR: A Hierarchical Interaction Framework via Goal-Driven Observation-Action Refinement for Robotic Manipulation](https://arxiv.org/abs/2511.17079v2)** | 2025-12-05 | <details><summary>Accep...</summary><p>Accepted to AAAI 2026 (Oral), Project Page: https://github.com/JiuTian-VL/H-GAR</p></details> |
| **[LLM-Driven Corrective Robot Operation Code Generation with Static Text-Based Simulation](https://arxiv.org/abs/2512.02002v2)** | 2025-12-05 | 8 pages, 2 figures |
| **[CLIO: A Tour Guide Robot with Co-speech Actions for Visual Attention Guidance and Enhanced User Engagement](https://arxiv.org/abs/2512.05389v1)** | 2025-12-05 | <details><summary>10 pa...</summary><p>10 pages, 7 figures, human-robot interaction</p></details> |
| **[WiSER-X: Wireless Signals-based Efficient Decentralized Multi-Robot Exploration without Explicit Information Exchange](https://arxiv.org/abs/2412.19876v2)** | 2025-12-04 |  |
| **[Disturbance Compensation for Safe Kinematic Control of Robotic Systems with Closed Architecture](https://arxiv.org/abs/2512.05292v1)** | 2025-12-04 |  |
| **[XR-DT: Extended Reality-Enhanced Digital Twin for Agentic Mobile Robots](https://arxiv.org/abs/2512.05270v1)** | 2025-12-04 | 10 pages, 5 figures |
| **[Invariance Co-training for Robot Visual Generalization](https://arxiv.org/abs/2512.05230v1)** | 2025-12-04 | 14 pages, 10 figures |
| **[From Generated Human Videos to Physically Plausible Robot Trajectories](https://arxiv.org/abs/2512.05094v1)** | 2025-12-04 | <details><summary>For p...</summary><p>For project website, see https://genmimic.github.io</p></details> |

## Vision Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Training-Time Action Conditioning for Efficient Real-Time Chunking](https://arxiv.org/abs/2512.05964v1)** | 2025-12-05 |  |
| **[M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG](https://arxiv.org/abs/2512.05959v1)** | 2025-12-05 | Preprint |
| **[iMotion-LLM: Instruction-Conditioned Trajectory Generation](https://arxiv.org/abs/2406.06211v3)** | 2025-12-05 |  |
| **[SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models](https://arxiv.org/abs/2512.05955v1)** | 2025-12-05 |  |
| **[TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.05943v1)** | 2025-12-05 |  |
| **[Zoom in, Click out: Unlocking and Evaluating the Potential of Zooming for GUI Grounding](https://arxiv.org/abs/2512.05941v1)** | 2025-12-05 | <details><summary>Code ...</summary><p>Code is available at https://github.com/Princeton-AI2-Lab/ZoomClick</p></details> |
| **[PRiSM: An Agentic Multimodal Benchmark for Scientific Reasoning via Python-Grounded Evaluation](https://arxiv.org/abs/2512.05930v1)** | 2025-12-05 |  |
| **[Open-PMC-18M: A High-Fidelity Large Scale Medical Dataset for Multimodal Representation Learning](https://arxiv.org/abs/2506.02738v3)** | 2025-12-05 | 21 pages |
| **[Uncovering Grounding IDs: How External Cues Shape Multimodal Binding](https://arxiv.org/abs/2509.24072v3)** | 2025-12-05 | <details><summary>Under...</summary><p>Under review as a conference paper at ICLR 2026</p></details> |
| **[Probing the effectiveness of World Models for Spatial Reasoning through Test-time Scaling](https://arxiv.org/abs/2512.05809v1)** | 2025-12-05 | <details><summary>Exten...</summary><p>Extended abstract at World Modeling Workshop 2026</p></details> |
| **[Distilling Expert Surgical Knowledge: How to train local surgical VLMs for anatomy explanation in Complete Mesocolic Excision](https://arxiv.org/abs/2512.05740v1)** | 2025-12-05 |  |
| **[Rethinking Sparse Autoencoders: Select-and-Project for Fairness and Control from Encoder Features Alone](https://arxiv.org/abs/2509.10809v2)** | 2025-12-05 |  |
| **[HiMoE-VLA: Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies](https://arxiv.org/abs/2512.05693v1)** | 2025-12-05 |  |
| **[Concept-Guided Backdoor Attack on Vision Language Models](https://arxiv.org/abs/2512.00713v2)** | 2025-12-05 |  |
| **[AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM](https://arxiv.org/abs/2503.04504v4)** | 2025-12-05 | <details><summary>Accep...</summary><p>Accepted to WACV 2026</p></details> |

## world model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Training-Time Action Conditioning for Efficient Real-Time Chunking](https://arxiv.org/abs/2512.05964v1)** | 2025-12-05 |  |
| **[AQUA-Net: Adaptive Frequency Fusion and Illumination Aware Network for Underwater Image Enhancement](https://arxiv.org/abs/2512.05960v1)** | 2025-12-05 |  |
| **[M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG](https://arxiv.org/abs/2512.05959v1)** | 2025-12-05 | Preprint |
| **[SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models](https://arxiv.org/abs/2512.05955v1)** | 2025-12-05 |  |
| **[Impugan: Learning Conditional Generative Models for Robust Data Imputation](https://arxiv.org/abs/2512.05950v1)** | 2025-12-05 |  |
| **[Designing an Optimal Sensor Network via Minimizing Information Loss](https://arxiv.org/abs/2512.05940v1)** | 2025-12-05 | <details><summary>37 pa...</summary><p>37 pages, 15 figures. Accepted to Bayesian Analysis</p></details> |
| **[Measuring the Effect of Background on Classification and Feature Importance in Deep Learning for AV Perception](https://arxiv.org/abs/2512.05937v1)** | 2025-12-05 | <details><summary>8 pag...</summary><p>8 pages, 2 figures, 7 tables</p></details> |
| **[Synset Signset Germany: a Synthetic Dataset for German Traffic Sign Recognition](https://arxiv.org/abs/2512.05936v1)** | 2025-12-05 | <details><summary>8 pag...</summary><p>8 pages, 8 figures, 3 tables</p></details> |
| **[On the Bayes Inconsistency of Disagreement Discrepancy Surrogates](https://arxiv.org/abs/2512.05931v1)** | 2025-12-05 | 37 pages, 7 figures |
| **[World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty](https://arxiv.org/abs/2512.05927v1)** | 2025-12-05 |  |
| **[From Text to Returns: Using Large Language Models for Mutual Fund Portfolio Optimization and Risk-Adjusted Allocation](https://arxiv.org/abs/2512.05907v1)** | 2025-12-05 |  |
| **[NeuroMemFPP: A recurrent neural approach for memory-aware parameter estimation in fractional Poisson process](https://arxiv.org/abs/2512.05893v1)** | 2025-12-05 | 12 pages |
| **[Variational Uncertainty Decomposition for In-Context Learning](https://arxiv.org/abs/2509.02327v3)** | 2025-12-05 | Neurips Version |
| **[SPARTAN: A Sparse Transformer World Model Attending to What Matters](https://arxiv.org/abs/2411.06890v3)** | 2025-12-05 |  |
| **[Vague Knowledge: Information without Transitivity and Partitions](https://arxiv.org/abs/2512.05833v1)** | 2025-12-05 |  |

