---
title: Latest 15 Papers - March 05, 2025
labels: documentation
---
**Please check the [Github](https://github.com/Ed1sonChen/DailyArxiv) page for a better reading experience and more papers.**

## MLLM
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[VOILA: Evaluation of MLLMs For Perceptual Understanding and Analogical Reasoning](http://arxiv.org/abs/2503.00043v2)** | 2025-03-04 | <details><summary>Accep...</summary><p>Accepted at ICLR 2025. Code and data: https://github.com/nlylmz/Voila</p></details> |
| **[MciteBench: A Benchmark for Multimodal Citation Text Generation in MLLMs](http://arxiv.org/abs/2503.02589v1)** | 2025-03-04 |  |
| **[From Specific-MLLMs to Omni-MLLMs: A Survey on MLLMs Aligned with Multi-modalities](http://arxiv.org/abs/2412.11694v3)** | 2025-03-04 | 35 pages |
| **[SV-RAG: LoRA-Contextualizing Adaptation of MLLMs for Long Document Understanding](http://arxiv.org/abs/2411.01106v2)** | 2025-03-02 | <details><summary>Accep...</summary><p>Accepted to ICLR 2025</p></details> |
| **[FunBench: Benchmarking Fundus Reading Skills of MLLMs](http://arxiv.org/abs/2503.00901v1)** | 2025-03-02 | 7 pages |
| **[OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference](http://arxiv.org/abs/2502.18411v2)** | 2025-03-01 |  |
| **[New Dataset and Methods for Fine-Grained Compositional Referring Expression Comprehension via Specialist-MLLM Collaboration](http://arxiv.org/abs/2502.20104v2)** | 2025-02-28 |  |
| **[Mitigating Object Hallucination in MLLMs via Data-augmented Phrase-level Alignment](http://arxiv.org/abs/2405.18654v3)** | 2025-02-28 | <details><summary>Publi...</summary><p>Published in ICLR 2025</p></details> |
| **[AsymLoRA: Harmonizing Data Conflicts and Commonalities in MLLMs](http://arxiv.org/abs/2502.20035v1)** | 2025-02-27 |  |
| **[Oryx MLLM: On-Demand Spatial-Temporal Understanding at Arbitrary Resolution](http://arxiv.org/abs/2409.12961v4)** | 2025-02-27 | <details><summary>Accep...</summary><p>Accepted to ICLR 2025</p></details> |
| **[Improving Adversarial Transferability in MLLMs via Dynamic Vision-Language Alignment Attack](http://arxiv.org/abs/2502.19672v1)** | 2025-02-27 | <details><summary>arXiv...</summary><p>arXiv admin note: text overlap with arXiv:2403.09766</p></details> |
| **[M2-omni: Advancing Omni-MLLM for Comprehensive Modality Support with Competitive Performance](http://arxiv.org/abs/2502.18778v1)** | 2025-02-26 |  |
| **[MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs](http://arxiv.org/abs/2502.17422v1)** | 2025-02-24 | <details><summary>Publi...</summary><p>Published as a conference paper at ICLR 2025. Code at: https://github.com/saccharomycetes/mllms_know</p></details> |
| **[MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation](http://arxiv.org/abs/2410.11779v2)** | 2025-02-23 | ICLR 2025 |
| **[M4SC: An MLLM-based Multi-modal, Multi-task and Multi-user Semantic Communication System](http://arxiv.org/abs/2502.16418v1)** | 2025-02-23 |  |

## Vision Language Action
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[A Survey on Vision-Language-Action Models for Embodied AI](http://arxiv.org/abs/2405.14093v4)** | 2025-03-04 | <details><summary>Proje...</summary><p>Project page: https://github.com/yueen-ma/Awesome-VLA</p></details> |
| **[Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding](http://arxiv.org/abs/2503.02310v1)** | 2025-03-04 |  |
| **[CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs](http://arxiv.org/abs/2503.01378v1)** | 2025-03-03 | <details><summary>Paper...</summary><p>Paper submitted to the IEEE conference</p></details> |
| **[A Taxonomy for Evaluating Generalist Robot Policies](http://arxiv.org/abs/2503.01238v1)** | 2025-03-03 | 25 pages |
| **[ObjectVLA: End-to-End Open-World Object Manipulation Without Demonstration](http://arxiv.org/abs/2502.19250v2)** | 2025-02-28 | <details><summary>Proje...</summary><p>Project page at https://objectvla.github.io/</p></details> |
| **[DexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping](http://arxiv.org/abs/2502.20900v1)** | 2025-02-28 | 22 pages, 10 figures |
| **[Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success](http://arxiv.org/abs/2502.19645v1)** | 2025-02-27 | <details><summary>Websi...</summary><p>Website: https://openvla-oft.github.io/</p></details> |
| **[Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models](http://arxiv.org/abs/2502.19417v1)** | 2025-02-26 |  |
| **[Evolution 6.0: Evolving Robotic Capabilities Through Generative Design](http://arxiv.org/abs/2502.17034v2)** | 2025-02-25 | Submitted to IROS |
| **[ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model](http://arxiv.org/abs/2502.14420v2)** | 2025-02-21 |  |
| **[VLAS: Vision-Language-Action Model With Speech Instructions For Customized Robot Manipulation](http://arxiv.org/abs/2502.13508v2)** | 2025-02-21 | <details><summary>Accep...</summary><p>Accepted as a conference paper at ICLR 2025</p></details> |
| **[CLIP-RT: Learning Language-Conditioned Robotic Policies from Natural Language Supervision](http://arxiv.org/abs/2411.00508v2)** | 2025-02-19 | 27 pages |
| **[NaVILA: Legged Robot Vision-Language-Action Model for Navigation](http://arxiv.org/abs/2412.04453v2)** | 2025-02-17 | <details><summary>Websi...</summary><p>Website: https://navila-bot.github.io/</p></details> |
| **[Survey on Vision-Language-Action Models](http://arxiv.org/abs/2502.06851v2)** | 2025-02-15 |  |
| **[HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation](http://arxiv.org/abs/2502.05485v3)** | 2025-02-14 | <details><summary>to be...</summary><p>to be published in ICLR 2025</p></details> |

