---
title: Latest 15 Papers - November 05, 2025
labels: documentation
---
**Please check the [Github](https://github.com/Ed1sonChen/DailyArxiv) page for a better reading experience and more papers.**

## Vision Language Action
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System](http://arxiv.org/abs/2511.02832v1)** | 2025-11-04 | <details><summary>Websi...</summary><p>Website: https://yanjieze.com/TWIST2</p></details> |
| **[XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations](http://arxiv.org/abs/2511.02776v1)** | 2025-11-04 |  |
| **[Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model](http://arxiv.org/abs/2510.27607v2)** | 2025-11-04 | 20 pages, 10 figures |
| **[DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment](http://arxiv.org/abs/2510.17148v4)** | 2025-11-04 |  |
| **[Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process](http://arxiv.org/abs/2511.01718v1)** | 2025-11-03 |  |
| **[Cosmos-Surg-dVRK: World Foundation Model-based Automated Online Evaluation of Surgical Robot Policy Learning](http://arxiv.org/abs/2510.16240v2)** | 2025-11-03 | <details><summary>minor...</summary><p>minor metadata and notation fixes; +3 citations</p></details> |
| **[PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model](http://arxiv.org/abs/2511.01571v1)** | 2025-11-03 | <details><summary>17pag...</summary><p>17pages,7 figures, 5 tabels</p></details> |
| **[UniVLA: Learning to Act Anywhere with Task-centric Latent Actions](http://arxiv.org/abs/2505.06111v3)** | 2025-11-03 | <details><summary>Accep...</summary><p>Accepted to RSS 2025. Code is available at https://github.com/OpenDriveLab/UniVLA</p></details> |
| **[RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models](http://arxiv.org/abs/2511.01331v1)** | 2025-11-03 |  |
| **[Embodiment Transfer Learning for Vision-Language-Action Models](http://arxiv.org/abs/2511.01224v1)** | 2025-11-03 |  |
| **[OmniVLA: Unifiying Multi-Sensor Perception for Physically-Grounded Multimodal VLA](http://arxiv.org/abs/2511.01210v1)** | 2025-11-03 |  |
| **[From Grounding to Manipulation: Case Studies of Foundation Model Integration in Embodied Robotic Systems](http://arxiv.org/abs/2505.15685v2)** | 2025-11-03 | <details><summary>EMNLP...</summary><p>EMNLP 2025 camera ready</p></details> |
| **[RoboOmni: Proactive Robot Manipulation in Omni-modal Context](http://arxiv.org/abs/2510.23763v3)** | 2025-11-01 |  |
| **[iFlyBot-VLA Technical Report](http://arxiv.org/abs/2511.01914v1)** | 2025-11-01 |  |
| **[World-Env: Leveraging World Model as a Virtual Environment for VLA Post-Training](http://arxiv.org/abs/2509.24948v3)** | 2025-11-01 |  |

## robot
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Mobile Robotic Multi-View Photometric Stereo](http://arxiv.org/abs/2502.10842v2)** | 2025-11-04 | <details><summary>Ackno...</summary><p>Acknowledgment Added. Published at International Society Journal of Photogrammetry and Remote Sensing (ISPRS). 32 pages, 14 Figures, 5 Tables</p></details> |
| **[FRASA: An End-to-End Reinforcement Learning Agent for Fall Recovery and Stand Up of Humanoid Robots](http://arxiv.org/abs/2410.08655v3)** | 2025-11-04 |  |
| **[Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation](http://arxiv.org/abs/2508.05635v3)** | 2025-11-04 | <details><summary>https...</summary><p>https://genie-envisioner.github.io/</p></details> |
| **[Dexterous Robotic Piano Playing at Scale](http://arxiv.org/abs/2511.02504v1)** | 2025-11-04 |  |
| **[From the Laboratory to Real-World Application: Evaluating Zero-Shot Scene Interpretation on Edge Devices for Mobile Robotics](http://arxiv.org/abs/2511.02427v1)** | 2025-11-04 | <details><summary>15 pa...</summary><p>15 pages, 6 figures, 1 table; accepted for AI-2025 Forty-fifth SGAI International Conference on Artificial Intelligence CAMBRIDGE, ENGLAND 16-18 DECEMBER 2025</p></details> |
| **[UniCoD: Enhancing Robot Policy via Unified Continuous and Discrete Representation Learning](http://arxiv.org/abs/2510.10642v2)** | 2025-11-04 |  |
| **[Neural Network Aided Kalman Filtering with Model Predictive Control Enables Robot-Assisted Drone Recovery on a Wavy Surface](http://arxiv.org/abs/2505.09145v3)** | 2025-11-04 | 17 pages, 51 figures |
| **[LACY: A Vision-Language Model-based Language-Action Cycle for Self-Improving Robotic Manipulation](http://arxiv.org/abs/2511.02239v1)** | 2025-11-04 | <details><summary>Prepr...</summary><p>Preprint. Project page: https://vla2026.github.io/LACY/</p></details> |
| **[Rethinking Bimanual Robotic Manipulation: Learning with Decoupled Interaction Framework](http://arxiv.org/abs/2503.09186v2)** | 2025-11-04 | 15 pages, 8 figures |
| **[RoboTron-Mani: All-in-One Multimodal Large Model for Robotic Manipulation](http://arxiv.org/abs/2412.07215v2)** | 2025-11-04 |  |
| **[A Quantitative Comparison of Centralised and Distributed Reinforcement Learning-Based Control for Soft Robotic Arms](http://arxiv.org/abs/2511.02192v1)** | 2025-11-04 | <details><summary>7 pag...</summary><p>7 pages, 4 figures, 2 tables, submitted to RoboSoft 2026</p></details> |
| **[Kinematic and Ergonomic Design of a Robotic Arm for Precision Laparoscopic Surgery](http://arxiv.org/abs/2511.02167v1)** | 2025-11-04 |  |
| **[Text to Robotic Assembly of Multi Component Objects using 3D Generative AI and Vision Language Models](http://arxiv.org/abs/2511.02162v1)** | 2025-11-04 | <details><summary>Accep...</summary><p>Accepted to NeurIPS 2025, Conference on Neural Information Processing Systems, Creative AI Track</p></details> |
| **[Census-Based Population Autonomy For Distributed Robotic Teaming](http://arxiv.org/abs/2511.02147v1)** | 2025-11-04 | 16 pages, 17 figures |
| **[Integrated Shape-Force Estimation for Continuum Robots: A Virtual-Work and Polynomial-Curvature Framework](http://arxiv.org/abs/2501.05418v3)** | 2025-11-03 |  |

## Vision Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations](http://arxiv.org/abs/2511.02776v1)** | 2025-11-04 |  |
| **[Prompt to Restore, Restore to Prompt: Cyclic Prompting for Universal Adverse Weather Removal](http://arxiv.org/abs/2503.09013v2)** | 2025-11-04 |  |
| **[Adapting General-Purpose Foundation Models for X-ray Ptychography in Low-Data Regimes](http://arxiv.org/abs/2511.02503v1)** | 2025-11-04 |  |
| **[MIP against Agent: Malicious Image Patches Hijacking Multimodal OS Agents](http://arxiv.org/abs/2503.10809v2)** | 2025-11-04 | NeurIPS 2025 |
| **[RxnCaption: Reformulating Reaction Diagram Parsing as Visual Prompt Guided Captioning](http://arxiv.org/abs/2511.02384v1)** | 2025-11-04 |  |
| **[From Flat to Hierarchical: Extracting Sparse Representations with Matching Pursuit](http://arxiv.org/abs/2506.03093v2)** | 2025-11-04 | <details><summary>39th ...</summary><p>39th Conference on Neural Information Processing Systems (NeurIPS 2025)</p></details> |
| **[The Pervasive Blind Spot: Benchmarking VLM Inference Risks on Everyday Personal Videos](http://arxiv.org/abs/2511.02367v1)** | 2025-11-04 |  |
| **[Unseen from Seen: Rewriting Observation-Instruction Using Foundation Models for Augmenting Vision-Language Navigation](http://arxiv.org/abs/2503.18065v3)** | 2025-11-04 | <details><summary>Accep...</summary><p>Accepted by IEEE Transactions on Neural Networks and Learning Systems</p></details> |
| **[CoCoVa: Chain of Continuous Vision-Language Thought for Latent Space Reasoning](http://arxiv.org/abs/2511.02360v1)** | 2025-11-04 |  |
| **[Grounded Vision-Language Interpreter for Integrated Task and Motion Planning](http://arxiv.org/abs/2506.03270v2)** | 2025-11-04 | <details><summary>Proje...</summary><p>Project website: https://omron-sinicx.github.io/ViLaIn-TAMP/</p></details> |
| **[LACY: A Vision-Language Model-based Language-Action Cycle for Self-Improving Robotic Manipulation](http://arxiv.org/abs/2511.02239v1)** | 2025-11-04 | <details><summary>Prepr...</summary><p>Preprint. Project page: https://vla2026.github.io/LACY/</p></details> |
| **[Dynamic Routing Between Experts: A Data-Efficient Approach to Continual Learning in Vision-Language Models](http://arxiv.org/abs/2511.01831v2)** | 2025-11-04 |  |
| **[ID-Composer: Multi-Subject Video Synthesis with Hierarchical Identity Preservation](http://arxiv.org/abs/2511.00511v2)** | 2025-11-04 |  |
| **[MetAdv: A Unified and Interactive Adversarial Testing Platform for Autonomous Driving](http://arxiv.org/abs/2508.06534v3)** | 2025-11-04 | <details><summary>ACM M...</summary><p>ACM MM 2025 Most Popular Demo Award</p></details> |
| **[DRIP: Dynamic patch Reduction via Interpretable Pooling](http://arxiv.org/abs/2510.25067v2)** | 2025-11-04 | Need more refinement |

## world model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Kosmos: An AI Scientist for Autonomous Discovery](http://arxiv.org/abs/2511.02824v1)** | 2025-11-04 |  |
| **[Agentic World Modeling for 6G: Near-Real-Time Generative State-Space Reasoning](http://arxiv.org/abs/2511.02748v1)** | 2025-11-04 | <details><summary>13 Pa...</summary><p>13 Pages, 3 Figures, 4 Tables</p></details> |
| **[Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model](http://arxiv.org/abs/2510.27607v2)** | 2025-11-04 | 20 pages, 10 figures |
| **[LTD-Bench: Evaluating Large Language Models by Letting Them Draw](http://arxiv.org/abs/2511.02347v1)** | 2025-11-04 | <details><summary>Accep...</summary><p>Accepted by NeurIPS 2025</p></details> |
| **[Communicating Plans, Not Percepts: Scalable Multi-Agent Coordination with Embodied World Models](http://arxiv.org/abs/2508.02912v3)** | 2025-11-04 | <details><summary>Publi...</summary><p>Published in the Proceedings of the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Scaling Environments for Agents (SEA). Additionally accepted for presentation in the NeurIPS 2025 Workshop: Embodied World Models for Decision Making (EWM) and the NeurIPS 2025 Workshop: Optimization for Machine Learning (OPT)</p></details> |
| **[The Physical Basis of Prediction: World Model Formation in Neural Organoids via an LLM-Generated Curriculum](http://arxiv.org/abs/2509.04633v3)** | 2025-11-04 | <details><summary>Publi...</summary><p>Published in the proceedings of the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Scaling Environments for Agents (SEA). Additionally accepted for presentation in NeurIPS 2025 Workshop: Embodied World Models for Decision Making</p></details> |
| **[Generative World Models of Tasks: LLM-Driven Hierarchical Scaffolding for Embodied Agents](http://arxiv.org/abs/2509.04731v3)** | 2025-11-04 | <details><summary>In th...</summary><p>In the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Embodied World Models for Decision Making (EWM)</p></details> |
| **[Learning Interactive World Model for Object-Centric Reinforcement Learning](http://arxiv.org/abs/2511.02225v1)** | 2025-11-04 | NeurIPS 2025 |
| **[Natural Building Blocks for Structured World Models: Theory, Evidence, and Scaling](http://arxiv.org/abs/2511.02091v1)** | 2025-11-03 | <details><summary>13 pa...</summary><p>13 pages, 3 figures, under review for World Modeling Workshop 2026</p></details> |
| **[How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment](http://arxiv.org/abs/2511.01775v1)** | 2025-11-03 |  |

