---
title: Latest 15 Papers - August 28, 2025
labels: documentation
---
**Please check the [Github](https://github.com/Ed1sonChen/DailyArxiv) page for a better reading experience and more papers.**

## Vision Language Action
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies](http://arxiv.org/abs/2508.20072v1)** | 2025-08-27 | 15 pages |
| **[Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation](http://arxiv.org/abs/2508.19958v1)** | 2025-08-27 | <details><summary>Accep...</summary><p>Accepted to CoRL 2025; Github Page: https://long-vla.github.io</p></details> |
| **[Ego-centric Predictive Model Conditioned on Hand Trajectories](http://arxiv.org/abs/2508.19852v1)** | 2025-08-27 | <details><summary>Code:...</summary><p>Code: github.com/binjiezhang/Ego-PM (branch: main)</p></details> |
| **[GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data](http://arxiv.org/abs/2505.03233v3)** | 2025-08-27 |  |
| **[MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation](http://arxiv.org/abs/2508.19236v1)** | 2025-08-26 | <details><summary>The p...</summary><p>The project is available at https://shihao1895.github.io/MemoryVLA</p></details> |
| **[FlowVLA: Thinking in Motion with a Visual Chain of Thought](http://arxiv.org/abs/2508.18269v2)** | 2025-08-26 |  |
| **[DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge](http://arxiv.org/abs/2507.04447v3)** | 2025-08-26 |  |
| **[MapleGrasp: Mask-guided Feature Pooling for Language-driven Efficient Robotic Grasping](http://arxiv.org/abs/2506.06535v3)** | 2025-08-25 |  |
| **[Continual Learning for Generative AI: From LLMs to MLLMs and Beyond](http://arxiv.org/abs/2506.13045v4)** | 2025-08-24 | Preprint |
| **[4D Visual Pre-training for Robot Learning](http://arxiv.org/abs/2508.17230v1)** | 2025-08-24 |  |
| **[GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions](http://arxiv.org/abs/2508.07650v2)** | 2025-08-23 | 10 pages, 6 figures |
| **[NinA: Normalizing Flows in Action. Training VLA Models with Normalizing Flows](http://arxiv.org/abs/2508.16845v1)** | 2025-08-23 |  |
| **[Do What? Teaching Vision-Language-Action Models to Reject the Impossible](http://arxiv.org/abs/2508.16292v1)** | 2025-08-22 | <details><summary>9 pag...</summary><p>9 pages, 2 figures, 1 table</p></details> |
| **[OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing](http://arxiv.org/abs/2508.08706v2)** | 2025-08-22 | <details><summary>15 pa...</summary><p>15 pages, 7 figures, 8 tables. ObjTac dataset: https://readerek.github.io/Objtac.github.io</p></details> |
| **[Survey of Vision-Language-Action Models for Embodied Manipulation](http://arxiv.org/abs/2508.15201v1)** | 2025-08-21 | in Chinese language |

## robot
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Discrete-Guided Diffusion for Scalable and Safe Multi-Robot Motion Planning](http://arxiv.org/abs/2508.20095v1)** | 2025-08-27 |  |
| **[HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation](http://arxiv.org/abs/2508.20085v1)** | 2025-08-27 |  |
| **[RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation](http://arxiv.org/abs/2506.18088v2)** | 2025-08-27 | <details><summary>Proje...</summary><p>Project Page: https://robotwin-platform.github.io/, Code: https://github.com/robotwin-Platform/robotwin, Doc: https://robotwin-platform.github.io/doc/</p></details> |
| **[Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation](http://arxiv.org/abs/2508.19958v1)** | 2025-08-27 | <details><summary>Accep...</summary><p>Accepted to CoRL 2025; Github Page: https://long-vla.github.io</p></details> |
| **[RoboComm: A DID-based scalable and privacy-preserving Robot-to-Robot interaction over state channels](http://arxiv.org/abs/2504.09517v3)** | 2025-08-27 | 19 pages, 10 figures |
| **[Walk the Robot: Exploring Soft Robotic Morphological Communication driven by Spiking Neural Networks](http://arxiv.org/abs/2508.19920v1)** | 2025-08-27 |  |
| **[A Standing Support Mobility Robot for Enhancing Independence in Elderly Daily Living](http://arxiv.org/abs/2508.19816v1)** | 2025-08-27 | <details><summary>7 pag...</summary><p>7 pages, accepted work for IEEE RO-MAN2025</p></details> |
| **[i2Nav-Robot: A Large-Scale Indoor-Outdoor Robot Dataset for Multi-Sensor Fusion Navigation and Mapping](http://arxiv.org/abs/2508.11485v2)** | 2025-08-27 | 10 pages, 12 figures |
| **[Context-Aware Risk Estimation in Home Environments: A Probabilistic Framework for Service Robots](http://arxiv.org/abs/2508.19788v1)** | 2025-08-27 | <details><summary>8 pag...</summary><p>8 pages, Accepted for IEEE RO-MAN 2025 Conference</p></details> |
| **[Efficient Human-Aware Task Allocation for Multi-Robot Systems in Shared Environments](http://arxiv.org/abs/2508.19731v1)** | 2025-08-27 | <details><summary>7 Pag...</summary><p>7 Pages, 4 Figures, Accepted in IROS2025</p></details> |
| **[Real-Time Sampling-Based Safe Motion Planning for Robotic Manipulators in Dynamic Environments](http://arxiv.org/abs/2501.00507v2)** | 2025-08-27 |  |
| **[Embodied Intelligence for Sustainable Flight: A Soaring Robot with Active Morphological Control](http://arxiv.org/abs/2508.19684v1)** | 2025-08-27 |  |
| **[From Tabula Rasa to Emergent Abilities: Discovering Robot Skills via Real-World Unsupervised Quality-Diversity](http://arxiv.org/abs/2508.19172v2)** | 2025-08-27 | <details><summary>Accep...</summary><p>Accepted at CoRL 2025</p></details> |
| **[AutoRing: Imitation Learning--based Autonomous Intraocular Foreign Body Removal Manipulation with Eye Surgical Robot](http://arxiv.org/abs/2508.19191v2)** | 2025-08-27 |  |
| **[A Lightweight Crowd Model for Robot Social Navigation](http://arxiv.org/abs/2508.19595v1)** | 2025-08-27 | <details><summary>7 pag...</summary><p>7 pages, 6 figures, accepted in ECMR 2025</p></details> |

## Vision Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Segmentation Assisted Incremental Test Time Adaptation in an Open World](http://arxiv.org/abs/2508.20029v1)** | 2025-08-27 | <details><summary>Accep...</summary><p>Accepted at BMVC 2025</p></details> |
| **[SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control](http://arxiv.org/abs/2508.20018v1)** | 2025-08-27 | 28 pages, 12 figures |
| **[GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity](http://arxiv.org/abs/2508.19972v1)** | 2025-08-27 |  |
| **[Assessing the Geolocation Capabilities, Limitations and Societal Risks of Generative Vision-Language Models](http://arxiv.org/abs/2508.19967v1)** | 2025-08-27 | <details><summary>Accep...</summary><p>Accepted to AAAI Fall Symposium 2025 on AI Trustworthiness and Risk Assessment for Challenging Contexts (ATRACC)</p></details> |
| **[KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts](http://arxiv.org/abs/2508.19944v1)** | 2025-08-27 |  |
| **[X-Prompt: Towards Universal In-Context Image Generation in Auto-Regressive Vision Language Foundation Models](http://arxiv.org/abs/2412.01824v2)** | 2025-08-27 | <details><summary>code:...</summary><p>code: https://github.com/SunzeY/X-Prompt</p></details> |
| **[Do Vision Encoders Truly Explain Object Hallucination?: Mitigating Object Hallucination via Simple Fine-Grained CLIPScore](http://arxiv.org/abs/2502.20034v3)** | 2025-08-27 |  |
| **[NLKI: A lightweight Natural Language Knowledge Integration Framework for Improving Small VLMs in Commonsense VQA Tasks](http://arxiv.org/abs/2508.19724v1)** | 2025-08-27 |  |
| **[Explain Before You Answer: A Survey on Compositional Visual Reasoning](http://arxiv.org/abs/2508.17298v2)** | 2025-08-27 | <details><summary>Proje...</summary><p>Project Page: https://github.com/pokerme7777/Compositional-Visual-Reasoning-Survey</p></details> |
| **[InquireMobile: Teaching VLM-based Mobile Agent to Request Human Assistance via Reinforcement Fine-Tuning](http://arxiv.org/abs/2508.19679v1)** | 2025-08-27 |  |
| **[R-TPT: Improving Adversarial Robustness of Vision-Language Models through Test-Time Prompt Tuning](http://arxiv.org/abs/2504.11195v2)** | 2025-08-27 | <details><summary>CVPR ...</summary><p>CVPR 2025 (Corrected the results on the Aircraft dataset)</p></details> |
| **[Self-Rewarding Vision-Language Model via Reasoning Decomposition](http://arxiv.org/abs/2508.19652v1)** | 2025-08-27 | <details><summary>16 pa...</summary><p>16 pages, two figures</p></details> |
| **[FakeSV-VLM: Taming VLM for Detecting Fake Short-Video News via Progressive Mixture-Of-Experts Adapter](http://arxiv.org/abs/2508.19639v1)** | 2025-08-27 | EMNLP2025 Findings |
| **[NPHardEval4V: Dynamic Evaluation of Large Vision-Language Models with Effects of Vision](http://arxiv.org/abs/2403.01777v3)** | 2025-08-27 | <details><summary>25 pa...</summary><p>25 pages, 9 figures, 2 tables</p></details> |
| **[Exploring Typographic Visual Prompts Injection Threats in Cross-Modality Generation Models](http://arxiv.org/abs/2503.11519v2)** | 2025-08-27 | <details><summary>This ...</summary><p>This paper is accepted by IJCAI2025 Workshop on Deepfake Detection, Localization, and Interpretability</p></details> |

## world model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[General agents contain world models](http://arxiv.org/abs/2506.01622v3)** | 2025-08-27 | <details><summary>Accep...</summary><p>Accepted ICML 2025. Typos corrected</p></details> |
| **[Tracking World States with Language Models: State-Based Evaluation Using Chess](http://arxiv.org/abs/2508.19851v1)** | 2025-08-27 | <details><summary>Spotl...</summary><p>Spotlight presentation at ICML 2025 Workshop on Assessing World Models</p></details> |
| **[Explain Before You Answer: A Survey on Compositional Visual Reasoning](http://arxiv.org/abs/2508.17298v2)** | 2025-08-27 | <details><summary>Proje...</summary><p>Project Page: https://github.com/pokerme7777/Compositional-Visual-Reasoning-Survey</p></details> |
| **[Enhancing Sample Efficiency and Exploration in Reinforcement Learning through the Integration of Diffusion Models and Proximal Policy Optimization](http://arxiv.org/abs/2409.01427v5)** | 2025-08-26 |  |
| **[MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation](http://arxiv.org/abs/2508.19320v1)** | 2025-08-26 | <details><summary>Techn...</summary><p>Technical Report. Project Page: https://chenmingthu.github.io/milm/</p></details> |
| **[FlowVLA: Thinking in Motion with a Visual Chain of Thought](http://arxiv.org/abs/2508.18269v2)** | 2025-08-26 |  |
| **[Dream to Chat: Model-based Reinforcement Learning on Dialogues with User Belief Modeling](http://arxiv.org/abs/2508.16876v2)** | 2025-08-26 | <details><summary>Accep...</summary><p>Accepted to EMNLP 2025 Findings</p></details> |
| **[Language Models For Generalised PDDL Planning: Synthesising Sound and Programmatic Policies](http://arxiv.org/abs/2508.18507v1)** | 2025-08-25 | <details><summary>RLC 2...</summary><p>RLC 2025 Workshop on Programmatic Reinforcement Learning</p></details> |
| **[ParticleFormer: A 3D Point Cloud World Model for Multi-Object, Multi-Material Robotic Manipulation](http://arxiv.org/abs/2506.23126v4)** | 2025-08-25 |  |
| **[Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI](http://arxiv.org/abs/2407.06886v8)** | 2025-08-25 | <details><summary>The c...</summary><p>The comprehensive review of Embodied AI. We also provide the resource repository for Embodied AI: https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List</p></details> |
| **[GWM: Towards Scalable Gaussian World Models for Robotic Manipulation](http://arxiv.org/abs/2508.17600v1)** | 2025-08-25 | <details><summary>Publi...</summary><p>Published at ICCV 2025. Project page: https://gaussian-world-model.github.io/</p></details> |
| **[HERO: Hierarchical Extrapolation and Refresh for Efficient World Models](http://arxiv.org/abs/2508.17588v1)** | 2025-08-25 | 12 pages in total |
| **[Seeing Clearly, Forgetting Deeply: Revisiting Fine-Tuned Video Generators for Driving Simulation](http://arxiv.org/abs/2508.16512v1)** | 2025-08-22 |  |
| **[WebEvolver: Enhancing Web Agent Self-Improvement with Coevolving World Model](http://arxiv.org/abs/2504.21024v2)** | 2025-08-21 | <details><summary>EMNLP...</summary><p>EMNLP 2025 Main Conference</p></details> |
| **[Goals and the Structure of Experience](http://arxiv.org/abs/2508.15013v1)** | 2025-08-20 |  |

