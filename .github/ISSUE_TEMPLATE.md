---
title: Latest 15 Papers - December 15, 2025
labels: documentation
---
**Please check the [Github](https://github.com/Ed1sonChen/DailyArxiv) page for a better reading experience and more papers.**

## Vision Language Action
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[BLURR: A Boosted Low-Resource Inference for Vision-Language-Action Models](https://arxiv.org/abs/2512.11769v1)** | 2025-12-12 | <details><summary>10 pa...</summary><p>10 pages, 3 figures. Code and integration scripts will be released at this http URL: https://github.com/JijiKing-Sam/BLURR-A-Boosted-Low-Resource-Inference-for-Vision-Language-Action-Model</p></details> |
| **[Embodied Image Compression](https://arxiv.org/abs/2512.11612v1)** | 2025-12-12 | <details><summary>15 pa...</summary><p>15 pages, 12 figures, 3 tables</p></details> |
| **[Atomic Action Slicing: Planner-Aligned Options for Generalist VLA Agents](https://arxiv.org/abs/2512.11584v1)** | 2025-12-12 | <details><summary>The 4...</summary><p>The 41st ACM/SIGAPP Symposium On Applied Computing</p></details> |
| **[Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning](https://arxiv.org/abs/2511.14396v3)** | 2025-12-12 | <details><summary>Accep...</summary><p>Accepted at AAAI 2026, the Project website is available at https://qhemu.github.io/CCoL/</p></details> |
| **[An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges](https://arxiv.org/abs/2512.11362v1)** | 2025-12-12 |  |
| **[Benchmarking the Generality of Vision-Language-Action Models](https://arxiv.org/abs/2512.11315v1)** | 2025-12-12 | <details><summary>23 pa...</summary><p>23 pages, 7 figures, and 1 table</p></details> |
| **[Seeing to Act, Prompting to Specify: A Bayesian Factorization of Vision Language Action Policy](https://arxiv.org/abs/2512.11218v1)** | 2025-12-12 |  |
| **[WholeBodyVLA: Towards Unified Latent VLA for Whole-Body Loco-Manipulation Control](https://arxiv.org/abs/2512.11047v1)** | 2025-12-11 |  |
| **[WAM-Flow: Parallel Coarse-to-Fine Motion Planning via Discrete Flow Matching for Autonomous Driving](https://arxiv.org/abs/2512.06112v2)** | 2025-12-11 | <details><summary>18 pa...</summary><p>18 pages, 11 figures. Code & Model: https://github.com/fudan-generative-vision/WAM-Flow</p></details> |
| **[RoboNeuron: A Modular Framework Linking Foundation Models and ROS for Embodied AI](https://arxiv.org/abs/2512.10394v1)** | 2025-12-11 |  |
| **[When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2511.16203v3)** | 2025-12-11 |  |
| **[Latent Chain-of-Thought World Modeling for End-to-End Driving](https://arxiv.org/abs/2512.10226v1)** | 2025-12-11 | Technical Report |
| **[HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models](https://arxiv.org/abs/2512.09928v1)** | 2025-12-10 | <details><summary>Proje...</summary><p>Project page: https://hifvla.github.io Github: https://github.com/OpenHelix-Team/HiF-VLA</p></details> |
| **[Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models](https://arxiv.org/abs/2512.09927v1)** | 2025-12-10 | 8 pages, 5 figures |
| **[UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving](https://arxiv.org/abs/2512.09864v1)** | 2025-12-10 | <details><summary>Proje...</summary><p>Project Page: https://seed-uniugp.github.io/</p></details> |

## robot
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis](https://arxiv.org/abs/2512.11797v1)** | 2025-12-12 | <details><summary>Proje...</summary><p>Project page: https://jay-ye.github.io/AnchorDream/</p></details> |
| **[ProbeMDE: Uncertainty-Guided Active Proprioception for Monocular Depth Estimation in Surgical Robotics](https://arxiv.org/abs/2512.11773v1)** | 2025-12-12 | 9 pages, 5 figures |
| **[The Influence of Human-like Appearance on Expected Robot Explanations](https://arxiv.org/abs/2512.11746v1)** | 2025-12-12 |  |
| **[Bench-Push: Benchmarking Pushing-based Navigation and Manipulation Tasks for Mobile Robots](https://arxiv.org/abs/2512.11736v1)** | 2025-12-12 | <details><summary>Under...</summary><p>Under review for ICRA 2026</p></details> |
| **[Iterative Compositional Data Generation for Robot Control](https://arxiv.org/abs/2512.10891v2)** | 2025-12-12 | <details><summary>Corre...</summary><p>Corrected reference chronological order and added acknowledgements; results unchanged</p></details> |
| **[Architecting Large Action Models for Human-in-the-Loop Intelligent Robots](https://arxiv.org/abs/2512.11620v1)** | 2025-12-12 |  |
| **[UniBYD: A Unified Framework for Learning Robotic Manipulation Across Embodiments Beyond Imitation of Human Demonstrations](https://arxiv.org/abs/2512.11609v1)** | 2025-12-12 |  |
| **[Mirror Skin: In Situ Visualization of Robot Touch Intent on Robotic Skin](https://arxiv.org/abs/2512.11472v1)** | 2025-12-12 |  |
| **[Fast Multi-Party Open-Ended Conversation with a Social Robot](https://arxiv.org/abs/2503.15496v2)** | 2025-12-12 | <details><summary>15 pa...</summary><p>15 pages, 5 figures, 4 tables; 2 appendices</p></details> |
| **[Real-Time QP Solvers: A Concise Review and Practical Guide Towards Legged Robots](https://arxiv.org/abs/2510.21773v2)** | 2025-12-12 | <details><summary>12 pa...</summary><p>12 pages, 1 figure, 2 tables</p></details> |
| **[Social Mediation through Robots -- A Scoping Review on Improving Group Interactions through Directed Robot Action using an Extended Group Process Model](https://arxiv.org/abs/2409.06557v2)** | 2025-12-12 | <details><summary>Early...</summary><p>Early version of the published journal paper: Weisswange, T. H., Javed, H., Dietrich, M., Jung, M. F., & Jamali, N. (2025). Design Implications for Robots that Facilitate Groups-A Scoping Review on Improving Group Interactions through Directed Robot Action. ACM Transactions on Human-Robot Interaction</p></details> |
| **[Bio-inspired reconfigurable stereo vision for robotics using omnidirectional cameras](https://arxiv.org/abs/2410.08691v2)** | 2025-12-12 | <details><summary>7 pag...</summary><p>7 pages, 8 figures, submitted to IEEE ICRA 2025</p></details> |
| **[Adaptive Compressive Tactile Subsampling: Enabling High Spatiotemporal Resolution in Scalable Robotic Skin](https://arxiv.org/abs/2410.13847v3)** | 2025-12-12 | <details><summary>51 pa...</summary><p>51 pages, 11 main figures, 16 supplemental figures, Videos can be accessed at https://tinyurl.com/ACTS-videos</p></details> |
| **[In-situ Value-aligned Human-Robot Interactions with Physical Constraints](https://arxiv.org/abs/2508.07606v2)** | 2025-12-12 | <details><summary>8 pag...</summary><p>8 pages, 7 figures. Accepted by IROS 2025</p></details> |
| **[Aligning Humans and Robots via Reinforcement Learning from Implicit Human Feedback](https://arxiv.org/abs/2507.13171v2)** | 2025-12-11 | <details><summary>Accep...</summary><p>Accepted to IEEE Int. Conf. Syst., Man, Cybern. (SMC) 2025</p></details> |

## Vision Language Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Extending a Parliamentary Corpus with MPs' Tweets: Automatic Annotation and Evaluation Using MultiParTweet](https://arxiv.org/abs/2512.11567v1)** | 2025-12-12 | <details><summary>Submi...</summary><p>Submitted to LREC 2026</p></details> |
| **[VLM2GeoVec: Toward Universal Multimodal Embeddings for Remote Sensing](https://arxiv.org/abs/2512.11490v1)** | 2025-12-12 | <details><summary>21 pa...</summary><p>21 pages, 7 figures, under review</p></details> |
| **[From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models](https://arxiv.org/abs/2512.10867v2)** | 2025-12-12 |  |
| **[Minimal Clips, Maximum Salience: Long Video Summarization via Key Moment Extraction](https://arxiv.org/abs/2512.11399v1)** | 2025-12-12 |  |
| **[The N-Body Problem: Parallel Execution from Single-Person Egocentric Video](https://arxiv.org/abs/2512.11393v1)** | 2025-12-12 | <details><summary>proje...</summary><p>project webpage: https://zhifanzhu.github.io/ego-nbody</p></details> |
| **[Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large Language Models and Beyond](https://arxiv.org/abs/2508.00522v2)** | 2025-12-12 |  |
| **[Surveillance Video-Based Traffic Accident Detection Using Transformer Architecture](https://arxiv.org/abs/2512.11350v1)** | 2025-12-12 |  |
| **[Benchmarking the Generality of Vision-Language-Action Models](https://arxiv.org/abs/2512.11315v1)** | 2025-12-12 | <details><summary>23 pa...</summary><p>23 pages, 7 figures, and 1 table</p></details> |
| **[The Finer the Better: Towards Granular-aware Open-set Domain Generalization](https://arxiv.org/abs/2511.16979v2)** | 2025-12-12 | <details><summary>9 pag...</summary><p>9 pages,3 figures,aaai2026</p></details> |
| **[Towards Logic-Aware Manipulation: A Knowledge Primitive for VLM-Based Assistants in Smart Manufacturing](https://arxiv.org/abs/2512.11275v1)** | 2025-12-12 | <details><summary>8 pag...</summary><p>8 pages, 2 figures, submitted to the 2026 IFAC World Congress</p></details> |
| **[Seeing to Act, Prompting to Specify: A Bayesian Factorization of Vision Language Action Policy](https://arxiv.org/abs/2512.11218v1)** | 2025-12-12 |  |
| **[Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language Alignment and Modeling](https://arxiv.org/abs/2505.17982v5)** | 2025-12-12 | <details><summary>Accep...</summary><p>Accepted at NeurIPS 2025</p></details> |
| **[Noise Matters: Optimizing Matching Noise for Diffusion Classifiers](https://arxiv.org/abs/2508.11330v2)** | 2025-12-12 |  |
| **[GoalLadder: Incremental Goal Discovery with Vision-Language Models](https://arxiv.org/abs/2506.16396v2)** | 2025-12-12 | NeurIPS 2025 |
| **[Image Tiling for High-Resolution Reasoning: Balancing Local Detail with Global Context](https://arxiv.org/abs/2512.11167v1)** | 2025-12-11 | <details><summary>Accep...</summary><p>Accepted in AAAI 2025 Workshop on Reproducible AI</p></details> |

## world model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis](https://arxiv.org/abs/2512.11797v1)** | 2025-12-12 | <details><summary>Proje...</summary><p>Project page: https://jay-ye.github.io/AnchorDream/</p></details> |
| **[WARPD: World model Assisted Reactive Policy Diffusion](https://arxiv.org/abs/2410.14040v4)** | 2025-12-12 | <details><summary>Outst...</summary><p>Outstanding Paper Award at the Embodied World Models for Decision Making Workshop at NeurIPS 2025</p></details> |
| **[FutureX: Enhance End-to-End Autonomous Driving via Latent Chain-of-Thought World Model](https://arxiv.org/abs/2512.11226v1)** | 2025-12-12 |  |
| **[VFMF: World Modeling by Forecasting Vision Foundation Model Features](https://arxiv.org/abs/2512.11225v1)** | 2025-12-12 |  |
| **[VDAWorld: World Modelling via VLM-Directed Abstraction and Simulation](https://arxiv.org/abs/2512.11061v1)** | 2025-12-11 | <details><summary>Websi...</summary><p>Website: https://felixomahony.github.io/vdaworld/</p></details> |
| **[WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World](https://arxiv.org/abs/2512.10958v1)** | 2025-12-11 | <details><summary>Prepr...</summary><p>Preprint; 80 pages, 37 figures, 29 tables; Project Page at https://worldbench.github.io/worldlens</p></details> |
| **[Generalized Spherical Neural Operators: Green's Function Formulation](https://arxiv.org/abs/2512.10723v1)** | 2025-12-11 |  |
| **[Evaluating Gemini Robotics Policies in a Veo World Simulator](https://arxiv.org/abs/2512.10675v1)** | 2025-12-11 |  |
| **[Object-centric proto-symbolic behavioural reasoning from pixels](https://arxiv.org/abs/2411.17438v3)** | 2025-12-11 | <details><summary>Accep...</summary><p>Accepted for publication in Neural Networks journal</p></details> |
| **[Orbis: Overcoming Challenges of Long-Horizon Prediction in Driving World Models](https://arxiv.org/abs/2507.13162v2)** | 2025-12-11 | <details><summary>Proje...</summary><p>Project page: https://lmb-freiburg.github.io/orbis.github.io/</p></details> |
| **[Thinking Ahead: Foresight Intelligence in MLLMs and World Models](https://arxiv.org/abs/2511.18735v2)** | 2025-12-11 | 25 pages, 27 figures |
| **[Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles](https://arxiv.org/abs/2512.03454v2)** | 2025-12-11 |  |
| **[Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks](https://arxiv.org/abs/2510.19195v3)** | 2025-12-11 |  |
| **[Latent Chain-of-Thought World Modeling for End-to-End Driving](https://arxiv.org/abs/2512.10226v1)** | 2025-12-11 | Technical Report |
| **[Latent Action World Models for Control with Unlabeled Trajectories](https://arxiv.org/abs/2512.10016v1)** | 2025-12-10 |  |

